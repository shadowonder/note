# æœºå™¨å­¦ä¹ 

åŸºäºpythonçš„machine learningç¬”è®°

ä½¿ç”¨åˆ°çš„åº“å’Œæ¡†æ¶: `Scikit learn`, `Tensorflow`

ç¯å¢ƒæ­å»º: `pip install Scikit-learn`

å¼•ç”¨: `import sklearn`

[toc]

## ç‰¹å¾æŠ½å–

ç‰¹å¾æŠ½å–å°±æ˜¯å°†æ•°æ®è¿›è¡Œç‰¹å¾åŒ–, æ•°å­—åŒ–

sklearnä¸­å­˜åœ¨ç€å¤§é‡çš„ç‰¹å¾æŠ½å–æ–¹æ³•. ä»¥å­—ç¬¦ä¸²ä¸ºä¾‹, æŠ½å–ä¸¤ä¸ªå­—ç¬¦ä¸²çš„æ–‡å­—ç‰¹å¾å±æ€§:

```python
from sklearn.feature_extraction.text import CountVectorizer

# å®ä¾‹åŒ–
vector = CountVectorizer()
# è½¬æ¢æ•°æ®
res = vector.fit_transform(["life is short , i like python", "life is too long, i dislike python"])

print(vector.get_feature_names())
# ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']

print(res.toarray())
# [[0 1 1 1 0 1 1 0]
#  [1 1 1 0 1 1 0 1]]

print(type(res))  # <class 'scipy.sparse._csr.csr_matrix'>
```

ç‰¹å¾æŠ½å–api: `sklearn.feature_extraction`

### å­—å…¸ç‰¹å¾æŠ½å–

* `DictVectorizer` : å­—å…¸æ•°æ®ç‰¹å¾æŠ½å– `DictVectorizer(sparse=True,...)`. å¦‚æœæ•°æ®å·²ç»æ˜¯æœ‰ç±»åˆ«çš„æ•°æ®, éœ€è¦å…ˆè½¬æ¢ä¸ºå­—å…¸ç±»å‹ç„¶åæ‰èƒ½è¯»å–æ•°æ®
  * fit_transform(x): å­—å…¸æˆ–è€…é¥±æ±‰å­ç‚¹çš„è¿­ä»£å™¨è½¬æ¢æˆä¸€ä¸ªsparseçŸ©é˜µ
  * inverse_transform(x): ä¼ å…¥arrayæ•°ç»„æˆ–è€…çŸ©é˜µ, è¿”å›è½¬æ¢ä¹‹å‰çš„æ•°æ®æ ¼å¼
  * get_feature_names(): è¿”å›ç±»åˆ«åç§°
  * transform(x): æŒ‰ç…§åŸå…ˆçš„æ ‡å‡†è½¬æ¢

```python
from sklearn.feature_extraction import DictVectorizer


def dictVec():
    """
    å­—å…¸æŠ½å–æ•°æ®
    :return:
    """
    # å®ä¾‹åŒ–
    dict = DictVectorizer(sparse=False)
    data = dict.fit_transform([{'city': 'åŒ—äº¬', 'temperature': 100},
                               {'city': 'ä¸Šæµ·', 'temperature': 60},
                               {'city': 'æ·±åœ³', 'temperature': 30}])
    print(data)
    # sparse = true
    #   (0, 1) 1.0
    #   (0, 3) 100.0
    #   (1, 0) 1.0
    #   (1, 3) 60.0
    #   (2, 2) 1.0
    #   (2, 3) 30.0
    # sparse = false , ä¹Ÿå°±æ˜¯ndarrayçš„ç±»å‹
    # ä¹Ÿè¢«ç§°ä¸ºone hotç¼–ç 
    # [[  0.   1.   0. 100.]
    #  [  1.   0.   0.  60.]
    #  [  0.   0.   1.  30.]]

    print(dict.inverse_transform(data))
    # è½¬æ¢æˆä¹‹å‰çš„æ•°æ®, ä½†æ˜¯è½¬æ¢æˆç‰¹å¾å€¼çš„ç±»å‹
    # [{'city=åŒ—äº¬': 1.0, 'temperature': 100.0}, {'city=ä¸Šæµ·': 1.0, 'temperature': 60.0}, {'city=æ·±åœ³': 1.0, 'temperature': 30.0}]

    print(dict.get_feature_names_out())  # ['city=ä¸Šæµ·', 'city=åŒ—äº¬', 'city=æ·±åœ³', 'temperature']
    return None


if __name__ == '__main__':
    dictVec()

```

### æ–‡æœ¬ç‰¹å¾æŠ½å–

* æ–‡æœ¬ç‰¹å¾æŠ½å–: `sklearn.feature_extration.text.CountVectorizer`
  * fit_transform(x) æ–‡æœ¬æˆ–è€…åŒ…å«å­—ç¬¦çš„å¯è¿­ä»£å¯¹è±¡è½¬æ¢æˆä¸€ä¸ªçŸ©é˜µè¾“å‡º
  * inverse_transform(x) æ•°ç»„æˆ–è€…sparseçŸ©é˜µè½¬æ¢æˆä¹‹å‰çš„æ•°æ®æ ¼å¼
  * get_feature_names() è·å–å•è¯åˆ—è¡¨
  * é»˜è®¤ä¸æ”¯æŒä¸­æ–‡éœ€è¦å®‰è£…jiebaè¿›è¡Œåˆ†è¯ç„¶åæ‰èƒ½è¿›è¡Œç»Ÿè®¡
    * `pip install jieba`

#### ç¬¬ä¸€ç§æ–¹æ³•

```python
def countVec():
    """
    å¯¹æ–‡æœ¬è¿›è¡Œç‰¹å¾å€¼åŒ–
    ç»Ÿè®¡æ‰€æœ‰æ–‡ç« çš„è¯, é‡å¤çš„åªè®¡ç®—ä¸€æ¬¡, ä½œä¸ºheaders
    é’ˆå¯¹è¿™ä¸ªåˆ—è¡¨, æ¯ä¸€ä¸ªæ–‡ç« ç»Ÿè®¡å•è¯ä¸ªæ•°, æ¯ä¸€ä¸ªæ–‡ç« ç»Ÿè®¡ä¸€æ¬¡, å¯¹äºå•ä¸ªå­—æ¯ä¸ç»Ÿè®¡(å­—æ¯ä¸ä¼šåæ˜ æ–‡ç« ä¸»é¢˜)

    é»˜è®¤ä¸æ”¯æŒä¸­æ–‡æŠ½å–, ä¼˜å…ˆè¿›è¡Œä¸­æ–‡åˆ†è¯
    :return: 
    """
    cv = CountVectorizer()

    data = cv.fit_transform(["life is is short,i like python", "life is too long,i dislike python"])
    print(data.toarray())
    # [[0 2 1 1 0 1 1 0]
    #  [1 1 1 0 1 1 0 1]]

    print(cv.get_feature_names_out())
    # ['dislike' 'is' 'life' 'like' 'long' 'python' 'short' 'too']

    return None


def cutword():
    con1 = jieba.cut("1ã€ä»Šå¤©å¾ˆæ®‹é…·ï¼Œæ˜å¤©æ›´æ®‹é…·ï¼Œåå¤©å¾ˆç¾å¥½ï¼Œä½†ç»å¯¹å¤§éƒ¨åˆ†æ˜¯æ­»åœ¨æ˜å¤©æ™šä¸Šï¼Œæ‰€ä»¥æ¯ä¸ªäººä¸è¦æ”¾å¼ƒä»Šå¤©ã€‚")
    con2 = jieba.cut("2ã€æˆ‘ä»¬çœ‹åˆ°çš„ä»å¾ˆè¿œæ˜Ÿç³»æ¥çš„å…‰æ˜¯åœ¨å‡ ç™¾ä¸‡å¹´ä¹‹å‰å‘å‡ºçš„ï¼Œè¿™æ ·å½“æˆ‘ä»¬çœ‹åˆ°å®‡å®™æ—¶ï¼Œæˆ‘ä»¬æ˜¯åœ¨çœ‹å®ƒçš„è¿‡å»ã€‚")
    con3 = jieba.cut("3ã€å¦‚æœåªç”¨ä¸€ç§æ–¹å¼äº†è§£æŸæ ·äº‹ç‰©ï¼Œä½ å°±ä¸ä¼šçœŸæ­£äº†è§£å®ƒã€‚äº†è§£äº‹ç‰©çœŸæ­£å«ä¹‰çš„ç§˜å¯†å–å†³äºå¦‚ä½•å°†å…¶ä¸æˆ‘ä»¬æ‰€äº†è§£çš„äº‹ç‰©ç›¸è”ç³»ã€‚")

    # è½¬æ¢æˆåˆ—è¡¨
    content1 = list(con1)
    print(content1)
    content2 = list(con2)
    print(content2)
    content3 = list(con3)
    print(content3)

    # è½¬æ¢æˆå­—ç¬¦ä¸²
    c1 = " ".join(content1)
    c2 = " ".join(content2)
    c3 = " ".join(content3)
    return c1, c2, c3


def chinese_vec():
    """
    ä¸­æ–‡æ–‡æœ¬æŠ½å–
    :return:
    """
    cv = CountVectorizer()
    c1, c2, c4 = cutword()
    data = cv.fit_transform([c1, c2, c4])
    print(cv.get_feature_names_out())
    # ['ä¸€ç§', 'ä¸ä¼š', 'ä¸è¦', 'ä¹‹å‰', 'äº†è§£', 'äº‹ç‰©', 'ä»Šå¤©', 'å…‰æ˜¯åœ¨', 'å‡ ç™¾ä¸‡å¹´', 'å‘å‡º', 'å–å†³äº', 'åªç”¨', 'åå¤©', 'å«ä¹‰', 'å¤§éƒ¨åˆ†', 'å¦‚ä½•', 'å¦‚æœ', 'å®‡å®™',
    #  'æˆ‘ä»¬', 'æ‰€ä»¥', 'æ”¾å¼ƒ', 'æ–¹å¼', 'æ˜å¤©', 'æ˜Ÿç³»', 'æ™šä¸Š', 'æŸæ ·', 'æ®‹é…·', 'æ¯ä¸ª', 'çœ‹åˆ°', 'çœŸæ­£', 'ç§˜å¯†', 'ç»å¯¹', 'ç¾å¥½', 'è”ç³»', 'è¿‡å»', 'è¿™æ ·']

    print(data.toarray())
    # [[0 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 0]
    #  [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 1]
    #  [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0]]
```

#### ç¬¬äºŒç§æ–¹æ³•

tfidfæ–‡æœ¬åˆ†ç±». æ–‡æœ¬ç‰¹å¾æŠ½å–åˆ†ç±».

* TF Term frequency, è¯é¢‘

$$tf_{i,j} = \frac{n_{i,j}}{\sum_kn_{k,j}}$$

* IDF inverse document frequency, é€†æ–‡æ¡£é¢‘ç‡
$$idf_i = lg\frac{|D|}{|\{j:t_i \in d_j\}|}$$
$|D|$ï¼šè¯­æ–™åº“ä¸­çš„æ–‡ä»¶æ€»æ•°
$|\{j:t_{i}\in d_{j}\}|$ï¼šåŒ…å«è¯è¯­$t_{{i}}$çš„æ–‡ä»¶æ•°ç›®ï¼ˆå³$n_{{i,j}}\neq 0$çš„æ–‡ä»¶æ•°ç›®ï¼‰å¦‚æœè¯è¯­ä¸åœ¨èµ„æ–™ä¸­ï¼Œå°±å¯¼è‡´åˆ†æ¯ä¸ºé›¶ï¼Œå› æ­¤ä¸€èˆ¬æƒ…å†µä¸‹ä½¿ç”¨${1+|\{j:t_{i}\in d_{j}\}|}$.

ä¹Ÿå°±æ˜¯log(æ€»æ–‡æ¡£æ•°/è¯¥è¯å‡ºç°çš„æ–‡æ¡£æ•°é‡)

sklearnçš„ç±»: `sklearn.feature_extraction.text.TfidfVectorizer`

```python
def tfidf_vec():
    tfidf_vector = TfidfVectorizer()
    c1, c2, c3 = cutword()
    data = tfidf_vector.fit_transform(["life is is short,i like python", "life is too long,i dislike python"])
    print(tfidf_vector.get_feature_names_out(data))
    print(data.toarray())
    # è¾“å‡ºç»“æœ -> æ•°å€¼è¡¨ç¤ºçš„æ˜¯é‡è¦æ€§
    # ['dislike' 'is' 'life' 'like' 'long' 'python' 'short' 'too']
    # [[0.         0.63402146 0.31701073 0.44554752 0.         0.31701073
    #   0.44554752 0.        ]
    #  [0.47042643 0.33471228 0.33471228 0.         0.47042643 0.33471228
    #   0.         0.47042643]]
```

### ç‰¹å¾é¢„å¤„ç† (æ•°æ®)

å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†: é€šè¿‡ç‰¹å®šçš„æ•°å­¦ç»Ÿè®¡æ–¹æ³•, å°†æ•°æ®è½¬æ¢ä¸ºç®—æ³•è¦æ±‚çš„æ•°æ®. ç›®çš„ä½¿å¾—ä¸€ä¸ªç‰¹å¾å¯¹æœ€ç»ˆç»“æœä¸ä¼šé€ æˆè¿‡å¤§çš„å½±å“.

![é¢„å¤„ç†](./images/Snipaste_2022-03-05_20-37-17.png)

é’ˆå¯¹äºsklearnçš„é¢„å¤„ç†æ–¹æ³•éƒ½å­˜å‚¨åœ¨`sklearn. preprocessing`ä¸­

#### å½’ä¸€åŒ– Normalization

å¦‚æœéœ€è¦å¤šä¸ªç‰¹å¾åŒç­‰é‡è¦çš„æ—¶å€™å°±å¯ä»¥ç”¨å½’ä¸€åŒ–. ä½†æ˜¯å½’ä¸€åŒ–å¯¹å¼‚å¸¸ç‚¹çš„å¤„ç†æ¯”è¾ƒå›°éš¾.

å…¬å¼:

$$X' = \frac{x - min}{max - min}$$ $$X'' = X' * (mx-mi) + mi$$

ä½œç”¨äºæ¯ä¸€åˆ—ï¼Œmaxä¸ºä¸€åˆ—çš„æœ€å¤§å€¼ï¼Œminä¸ºä¸€åˆ—çš„æœ€å°å€¼,é‚£ä¹ˆXâ€™â€™
ä¸ºæœ€ç»ˆç»“æœï¼Œmxï¼Œmiåˆ†åˆ«ä¸ºæŒ‡å®šåŒºé—´å€¼é»˜è®¤mxä¸º1,miä¸º0

ç›¸å¯¹äºä¸Šå›¾ä¸­ç¬¬ä¸€ç»„æ•°æ®çš„ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—å’Œç¬¬ä¸€è¡Œç¬¬äºŒåˆ—çš„æ•°æ®ä¸º
![è®¡ç®—](./images/2.png)

**sklearnå½’ä¸€åŒ–API:  `sklearn.preprocessing.MinMaxScaler`**

```python
from sklearn.preprocessing import MinMaxScaler
def normalization():
    """
    å½’ä¸€åŒ–
    :return:
    """
    normalizer = MinMaxScaler()
    data = normalizer.fit_transform([[90, 2, 10, 40],
                                     [60, 4, 15, 45],
                                     [75, 3, 13, 46]])
    print(data)
    # [[1.         0.         0.         0.        ]
    #  [0.         1.         1.         0.83333333]
    #  [0.5        0.5        0.6        1.        ]]

    normalizer = MinMaxScaler(feature_range=(2, 3))
    data = normalizer.fit_transform([[90, 2, 10, 40],
                                     [60, 4, 15, 45],
                                     [75, 3, 13, 46]])
    print(data)
    # [[3.         2.         2.         2.        ]
    #  [2.         3.         3.         2.83333333]
    #  [2.5        2.5        2.6        3.        ]]
    return None
```

*æ³¨æ„åœ¨ç‰¹å®šåœºæ™¯ä¸‹æœ€å¤§å€¼æœ€å°å€¼æ˜¯å˜åŒ–çš„ï¼Œå¦å¤–ï¼Œæœ€å¤§å€¼ä¸æœ€å°å€¼éå¸¸å®¹æ˜“å—å¼‚å¸¸ç‚¹å½±å“ï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•é²æ£’æ€§è¾ƒå·®ï¼Œåªé€‚åˆä¼ ç»Ÿç²¾ç¡®å°æ•°æ®åœºæ™¯ã€‚*

---

#### æ ‡å‡†åŒ–

é€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®å˜æ¢åˆ°å‡å€¼ä¸º0,æ–¹å·®ä¸º1èŒƒå›´å†…

**å…¬å¼:**

$$X' = \frac{x - mean}{\sigma}$$

ä½œç”¨äºæ¯ä¸€åˆ—ï¼Œmeanä¸ºå¹³å‡å€¼ï¼Œğœä¸ºæ ‡å‡†å·®(è€ƒé‡æ•°æ®çš„ç¨³å®šæ€§)

stdæˆä¸ºæ–¹å·®:

$$ğ‘ ğ‘¡ğ‘‘=\frac{((ğ‘¥1âˆ’ğ‘šğ‘’ğ‘ğ‘›)^2+(ğ‘¥2âˆ’ğ‘šğ‘’ğ‘ğ‘›)^2+â€¦)}{(ğ‘›(æ¯ä¸ªç‰¹å¾çš„æ ·æœ¬æ•°))}ï¼Œ\sigma = \sqrt{std}$$

sklearnç‰¹å¾åŒ–API: `scikit-learn.preprocessing.StandardScaler`

```python
from sklearn.preprocessing import StandardScaler
def standarlization():
    """
    æ ‡å‡†åŒ–ç¼©æ”¾
    :return:
    """
    std = StandardScaler()
    data = std.fit_transform([[1., -1., 3.],
                              [2., 4., 2.],
                              [4., 6., -1.]])
    print(data)
    # [[-1.06904497 -1.35873244  0.98058068]
    #  [-0.26726124  0.33968311  0.39223227]
    #  [ 1.33630621  1.01904933 -1.37281295]]
    return None
```

#### ç¼ºå¤±å€¼çš„å¤„ç†æ–¹æ³•

å¦‚æœæ¯åˆ—æˆ–è€…è¡Œæ•°æ®ç¼ºå¤±å€¼è¾¾åˆ°ä¸€å®šçš„æ¯”ä¾‹ï¼Œå»ºè®®æ”¾å¼ƒæ•´è¡Œæˆ–è€…æ•´åˆ—

å¯ä»¥é€šè¿‡ç¼ºå¤±å€¼æ¯è¡Œæˆ–è€…æ¯åˆ—çš„å¹³å‡å€¼ã€ä¸­ä½æ•°æ¥å¡«å……

sklearnç¼ºå¤±å€¼API: `sklearn.preprocessing.Imputer`

*æ³¨æ„: æ–°ç‰ˆæœ¬ä¸­ä¸å­˜åœ¨è¿™ä¸ªç±», åœ¨ä½ç‰ˆæœ¬ä¸­å­˜åœ¨, åŒ…ç‰ˆæœ¬ä¸­å¯ä»¥ä½¿ç”¨SimpleImputer. åŒæ—¶sklearn.imputeä¸­å­˜åœ¨å…¶ä»–çš„imputerç±»å¯ä»¥ä½¿ç”¨*

```python
from sklearn.impute import SimpleImputer
def imputer():
    """
    ç¼ºå¤±å€¼å¤„ç†
    :return:
    """
    # æ›¿æ¢ç­–ç•¥:
    # "mean"ï¼Œä½¿ç”¨è¯¥åˆ—çš„å¹³å‡å€¼æ›¿æ¢ç¼ºå¤±å€¼ã€‚ä»…ç”¨äºæ•°å€¼æ•°æ®ï¼›
    # "median"ï¼Œä½¿ç”¨è¯¥åˆ—çš„ä¸­ä½æ•°æ›¿æ¢ç¼ºå¤±å€¼ã€‚ä»…ç”¨äºæ•°å€¼æ•°æ®ï¼›
    # "most_frequent"ï¼Œä½¿ç”¨æ¯ä¸ªåˆ—ä¸­æœ€å¸¸è§çš„å€¼æ›¿æ¢ç¼ºå¤±å€¼ã€‚å¯ç”¨äºéæ•°å€¼æ•°æ®ï¼›
    # "constant"ï¼Œç”¨fill_valueæ›¿æ¢ç¼ºå¤±å€¼ã€‚å¯ç”¨äºéæ•°å€¼æ•°æ®
    imp = SimpleImputer(missing_values=np.nan, strategy='mean')
    data = imp.fit_transform([[1, 2],
                              [np.nan, 3],
                              [7, 6]])
    print(data)
    # [[1. 2.]
    #  [4. 3.]
    #  [7. 6.]]
    return None
```

#### æ•°æ®é™ç»´

æ•°æ®é™ç»´çš„æ„æ€æ˜¯å‡å°‘æ•°æ®ç‰¹å¾çš„æ•°é‡

ç‰¹å¾é™ç»´çš„åŸå› :

* å†—ä½™ï¼šéƒ¨åˆ†ç‰¹å¾çš„ç›¸å…³åº¦é«˜ï¼Œå®¹æ˜“æ¶ˆè€—è®¡ç®—æ€§èƒ½

* å™ªå£°ï¼šéƒ¨åˆ†ç‰¹å¾å¯¹é¢„æµ‹ç»“æœæœ‰è´Ÿå½±å“

**æ–¹å·®è¿‡æ»¤**

ç‰¹å¾è¿‡æ»¤çš„æ–¹æ³• `sklearn.feature_selection.VarianceThreshold`

```python
def variance():
    """
    åˆ é™¤ä½æ–¹å·®çš„ç‰¹å¾
    """
    var = VarianceThreshold(threshold=0.00001)  # åˆ é™¤æ–¹å·®ä½äº0.00001çš„æ•°æ®
    data = var.fit_transform([[0, 2, 0, 3],
                              [0, 1, 4, 3],
                              [0, 1, 1, 3]])
    print(data)
    # [[2 0]
    #  [1 4]
    #  [1 1]]
    return None
```

**ä¸»æˆåˆ†åˆ†æ (PCA)**

æœ¬è´¨ï¼šPCAæ˜¯ä¸€ç§åˆ†æã€ç®€åŒ–æ•°æ®é›†çš„æŠ€æœ¯

ç›®çš„ï¼šæ˜¯æ•°æ®ç»´æ•°å‹ç¼©ï¼Œå°½å¯èƒ½é™ä½åŸæ•°æ®çš„ç»´æ•°ï¼ˆå¤æ‚åº¦ï¼‰ï¼ŒæŸå¤±å°‘é‡ä¿¡æ¯ã€‚

ä½œç”¨ï¼šå¯ä»¥å‰Šå‡å›å½’åˆ†ææˆ–è€…èšç±»åˆ†æä¸­ç‰¹å¾çš„æ•°é‡

PCAçš„æ•°å­¦å®šä¹‰æ˜¯ï¼šä¸€ä¸ªæ­£äº¤åŒ–çº¿æ€§å˜æ¢ï¼ŒæŠŠæ•°æ®å˜æ¢åˆ°ä¸€ä¸ªæ–°çš„åæ ‡ç³»ç»Ÿä¸­ï¼Œä½¿å¾—è¿™ä¸€æ•°æ®çš„ä»»ä½•æŠ•å½±çš„ç¬¬ä¸€å¤§æ–¹å·®åœ¨ç¬¬ä¸€ä¸ªåæ ‡ï¼ˆç§°ä¸ºç¬¬ä¸€ä¸»æˆåˆ†ï¼‰ä¸Šï¼Œç¬¬äºŒå¤§æ–¹å·®åœ¨ç¬¬äºŒä¸ªåæ ‡ï¼ˆç¬¬äºŒä¸»æˆåˆ†ï¼‰ä¸Šï¼Œä¾æ¬¡ç±»æ¨

n_components: ä¸€èˆ¬ç”¨å°æ•°, è¯´æ˜ä¿ç•™çš„ç‰¹å¾æ•°æ®. æŠŠéƒ¨åˆ†æ•°æ®å‡å°‘åˆ°90%. å¦‚æœä½¿ç”¨æ•´æ•°çš„è¯, å°±è¡¨æ˜éœ€è¦é™åˆ°å¤šå°‘ä¸ªç‰¹å¾, ä¸€èˆ¬ä¸ç”¨

```python
def pca():
    """
    ä¸»æˆåˆ†åˆ†æè¿›è¡Œæ•°æ®é™ç»´
    :return:
    """
    p = PCA(n_components=0.9)
    data = p.fit_transform([[2, 8, 4, 5],
                            [6, 3, 0, 8],
                            [5, 4, 9, 1]])
    print(data)
    # [[ 1.28620952e-15  3.82970843e+00]
    #  [ 5.74456265e+00 -1.91485422e+00]
    #  [-5.74456265e+00 -1.91485422e+00]]
    return None
```

## æœºå™¨å­¦ä¹ æ¨¡å‹

é’ˆå¯¹ä¸åŒçš„æ•°æ®éœ€è¦ä½¿ç”¨ä¸åŒçš„æ•°æ®ç±»å‹. *åªè¦è®°ä½ä¸€ç‚¹ï¼Œç¦»æ•£å‹æ˜¯åŒºé—´å†…ä¸å¯åˆ†ï¼Œè¿ç»­å‹æ˜¯åŒºé—´å†…å¯åˆ†*

* ç¦»æ•£å‹æ•°æ®ï¼šç”±è®°å½•ä¸åŒç±»åˆ«ä¸ªä½“çš„æ•°ç›®æ‰€å¾—åˆ°çš„æ•°æ®ï¼Œåˆç§°è®¡æ•°æ•°æ®ï¼Œæ‰€æœ‰è¿™äº›æ•°æ®å…¨éƒ¨éƒ½æ˜¯æ•´æ•°ï¼Œè€Œä¸”ä¸èƒ½å†ç»†åˆ†ï¼Œä¹Ÿä¸èƒ½è¿›ä¸€æ­¥æé«˜ä»–ä»¬çš„ç²¾ç¡®åº¦ã€‚
* è¿ç»­å‹æ•°æ®ï¼šå˜é‡å¯ä»¥åœ¨æŸä¸ªèŒƒå›´å†…å–ä»»ä¸€æ•°ï¼Œå³å˜é‡çš„å–å€¼å¯ä»¥æ˜¯è¿ç»­çš„ï¼Œå¦‚ï¼Œé•¿åº¦ã€æ—¶é—´ã€è´¨é‡å€¼ç­‰ï¼Œè¿™ç±»æ•´æ•°é€šå¸¸æ˜¯éæ•´æ•°ï¼Œå«æœ‰å°æ•°éƒ¨åˆ†ã€‚

æœºå™¨å­¦ä¹ çš„åˆ†ç±»

* ç›‘ç£å­¦ä¹  (ç‰¹å¾å€¼åŠ ç›®æ ‡å€¼)
  * åˆ†ç±»(ç›®æ ‡å€¼ç¦»æ•£å‹æ•°æ®) k-è¿‘é‚»ç®—æ³•ã€è´å¶æ–¯åˆ†ç±»ã€å†³ç­–æ ‘ä¸éšæœºæ£®æ—ã€é€»è¾‘å›å½’ã€ç¥ç»ç½‘ç»œ
  * å›å½’(ç›®æ ‡å€¼è¿ç»­å‹æ•°æ®) çº¿æ€§å›å½’ã€å²­å›å½’
  * æ ‡æ³¨ éšé©¬å°”å¯å¤«æ¨¡å‹
* æ— ç›‘ç£å­¦ä¹  (åªæœ‰ç‰¹å¾å€¼)
  * èšç±» k-means

1. æ˜ç¡®æ•°æ®éœ€è¦åšä»€ä¹ˆ
2. å¤„ç†æ•°æ®
3. ç‰¹å¾å·¥ç¨‹, å¤„ç†ç‰¹å¾
4. æ‰¾åˆ°åˆé€‚çš„ç®—æ³•è¿›è¡Œé¢„æµ‹
5. è¯„ä¼°è®¡ç®—ç»“æœæ¨¡å‹ (å¦‚æœæˆåŠŸ,é‚£ä¹ˆéƒ¨ç½²,å¦‚æœæ²¡æˆåŠŸ,å¯ä»¥è°ƒæ•´å‚æ•°æˆ–è€…æ¢ç®—æ³•,é‡æ–°å°è¯•ç‰¹å¾å·¥ç¨‹)
6. ä½¿ç”¨, ä»¥apiå½¢å¼æä¾›

ç›‘ç£å­¦ä¹ ï¼ˆè‹±è¯­ï¼šSupervised learningï¼‰ï¼Œå¯ä»¥ç”±è¾“å…¥æ•°æ®ä¸­å­¦åˆ°æˆ–å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä¾æ­¤æ¨¡å¼æ¨æµ‹æ–°çš„ç»“æœã€‚è¾“å…¥æ•°æ®æ˜¯ç”±è¾“å…¥ç‰¹å¾å€¼å’Œç›®æ ‡å€¼æ‰€ç»„æˆã€‚å‡½æ•°çš„è¾“å‡ºå¯ä»¥æ˜¯ä¸€ä¸ªè¿ç»­çš„å€¼ï¼ˆç§°ä¸ºå›å½’ï¼‰ï¼Œæˆ–æ˜¯è¾“å‡ºæ˜¯æœ‰é™ä¸ªç¦»æ•£å€¼ï¼ˆç§°ä½œåˆ†ç±»ï¼‰ã€‚

æ— ç›‘ç£å­¦ä¹ ï¼ˆè‹±è¯­ï¼šSupervised learningï¼‰ï¼Œå¯ä»¥ç”±è¾“å…¥æ•°æ®ä¸­å­¦åˆ°æˆ–å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä¾æ­¤æ¨¡å¼æ¨æµ‹æ–°çš„ç»“æœã€‚è¾“å…¥æ•°æ®æ˜¯ç”±è¾“å…¥ç‰¹å¾å€¼æ‰€ç»„æˆã€‚

### æ•°æ®åˆ’åˆ†

é€šå¸¸æƒ…å†µä¸‹è®­ç»ƒé›†çš„æ•°æ®å’Œæµ‹è¯•é›†çš„æ•°æ®åˆ’åˆ†ä¸º7:3æˆ–è€…3:1. å¯ä»¥ä½¿ç”¨pythonæ ·æœ¬åˆ’åˆ†å·¥å…·.

æ•°æ®é›†åˆ’åˆ†api `sklearn.model_selection.train_test_split`

load*å’Œfetch*è¿”å›çš„æ•°æ®ç±»å‹datasets.base.Bunch(å­—å…¸æ ¼å¼)

* dataï¼šç‰¹å¾æ•°æ®æ•°ç»„ï¼Œæ˜¯ [n_samples * n_features] çš„äºŒç»´ numpy.ndarray æ•°ç»„
* targetï¼šæ ‡ç­¾æ•°ç»„ï¼Œæ˜¯ n_samples çš„ä¸€ç»´ numpy.ndarray æ•°ç»„
* DESCRï¼šæ•°æ®æè¿°
* feature_namesï¼šç‰¹å¾å,æ–°é—»æ•°æ®ï¼Œæ‰‹å†™æ•°å­—ã€å›å½’æ•°æ®é›†æ²¡æœ‰
* target_namesï¼šæ ‡ç­¾å,å›å½’æ•°æ®é›†æ²¡æœ‰

åˆ†ç±»æ•°æ®é›†çš„æ ¼å¼

sklearn.datasets.load_iris() åŠ è½½å¹¶è¿”å›é¸¢å°¾èŠ±æ•°æ®é›†, ä¸€ç»„æµ‹è¯•æ•°æ®. åˆ†ç±»ç±»å‹æ•°æ®

```python
from sklearn.datasets import load_iris

li = load_iris()
print("è·å–ç‰¹å¾å€¼")
print(li.data)
# è·å–ç‰¹å¾å€¼
# [[5.1 3.5 1.4 0.2]
#  [4.9 3.  1.4 0.2]
#  [4.7 3.2 1.3 0.2]
# ...

print("ç›®æ ‡å€¼")
print(li.target)
# ç›®æ ‡å€¼
# [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
#  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
#  2 2]

print(li.DESCR)
# å±•ç¤ºèŠ±çš„ç‰¹å¾
```

sklearn.datasets.load_digits() åŠ è½½å¹¶è¿”å›æ•°å­—æ•°æ®é›†

#### æ•°æ®é›†åˆ†å‰²

API: `sklearn.model_selection.train_test_split(*arrays,Â **options)`

* x æ•°æ®é›†çš„ç‰¹å¾å€¼
* y æ•°æ®é›†çš„æ ‡ç­¾å€¼
* test_size æµ‹è¯•é›†çš„å¤§å°ï¼Œä¸€èˆ¬ä¸ºfloat
* random_state éšæœºæ•°ç§å­,ä¸åŒçš„ç§å­ä¼šé€ æˆä¸åŒçš„éšæœºé‡‡æ ·ç»“æœã€‚ç›¸åŒçš„ç§å­é‡‡æ ·ç»“æœç›¸åŒã€‚
* return  è®­ç»ƒé›†ç‰¹å¾å€¼ï¼Œæµ‹è¯•é›†ç‰¹å¾å€¼ï¼Œè®­ç»ƒæ ‡ç­¾ï¼Œæµ‹è¯•æ ‡ç­¾ (é»˜è®¤éšæœºå–)

```python
# è¿”å›å€¼:è®­ç»ƒé›†x_train,y_train. æµ‹è¯•é›†x_test,y_test
x_train, x_test, y_train, y_test = train_test_split(li.data, li.target, test_size=0.25)
print("è®­ç»ƒé›†ç‰¹å¾å€¼å’Œç›®æ ‡å€¼", x_train, y_train)
print("æµ‹è¯•é›†ç‰¹å¾å€¼å’Œç›®æ ‡å€¼", x_test, y_test)
```

ä¸‹è½½ä¸€ä¸ªç”¨äºåˆ†ç±»çš„å¤§æ•°æ®é›†, è¿™é‡Œä¼šä¸‹è½½ä¸€ä¸ªæµ‹è¯•æ•°æ®é›†åœ¨pythonçš„homeç›®å½•ä¸­(*æ³¨æ„:æ•°æ®é‡è¶…å¤§, æ²¡äº‹åˆ«æµª*):

`sklearn.datasets.fetch_20newsgroups(data_home=None,subset=â€˜trainâ€™)`

subset: 'train'æˆ–è€…'test','all'ï¼Œå¯é€‰ï¼Œé€‰æ‹©è¦åŠ è½½çš„æ•°æ®é›†.è®­ç»ƒé›†çš„â€œè®­ç»ƒâ€ï¼Œæµ‹è¯•é›†çš„â€œæµ‹è¯•â€ï¼Œä¸¤è€…çš„â€œå…¨éƒ¨â€

å¯ä»¥ä½¿ç”¨å‘½ä»¤`datasets.clear_data_home(data_home=None)`æ¥æ¸…é™¤ç›®å½•ä¸‹çš„æ•°æ®

```python
news = fetch_20newsgroups(subset='all')
print(news.data)
print(news.target)
```

ä¸‹è½½ä¸€ä¸ªç”¨äºå›å½’çš„å¤§æ•°æ®é›†

`sklearn.datasets.load_boston()` åŠ è½½å¹¶è¿”å›æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†
`sklearn.datasets.load_diabetes()` åŠ è½½å’Œè¿”å›ç³–å°¿ç—…æ•°æ®é›†

```python
lb = load_boston()
print(lb.data)  # ç‰¹å¾å€¼
print(lb.target)  # ç›®æ ‡å€¼
print(lb.DESCR)
```

#### è½¬æ¢å™¨, é¢„ä¼°å™¨

**è½¬æ¢å™¨**

```python
from sklearn.preprocessing import StandardScaler

s = StandardScaler()
s.fit_transform([[1, 2, 3], [4, 5, 6]])

ss = StandardScaler();
ss.fit([[1, 2, 3], [4, 5, 6]])
print(ss.transform([[1, 2, 3], [4, 5, 6]]))
# [[-1. -1. -1.]
#  [ 1.  1.  1.]]

# fit_transform = fit + transform

ss.fit([[1, 2, 3], [4, 5, 7]])  # æ­¤å¤„è¿ç®—çš„æ ‡å‡†å·®å’Œæ–¹å·®
print(ss.transform([[1, 2, 3], [4, 5, 6]]))  # ç”±äºæ ‡å‡†å·®fitè®¡ç®—å‡ºæ¥çš„ä¸ä¸€æ ·,å› æ­¤ç»“æœä¸åŒ
# [[-1.  -1.  -1. ]
#  [ 1.   1.   0.5]]

# ä¹Ÿå¯ä»¥é€šè¿‡æ•°æ®åˆ‡åˆ†åˆ’åˆ†æ•°æ®
# ç¼©å°æ•°æ®, é€šè¿‡queryæŸ¥è¯¢æ•°æ®
data = data.query("x > 1.0 & x < 1.25 & y > 2.5 & < 2.75")
```

**ä¼°è®¡å™¨**

åœ¨sklearnä¸­ï¼Œä¼°è®¡å™¨(estimator)æ˜¯ä¸€ä¸ªé‡è¦çš„è§’è‰²ï¼Œåˆ†ç±»å™¨å’Œå›å½’å™¨éƒ½å±äºestimatorï¼Œæ˜¯ä¸€ç±»å®ç°äº†ç®—æ³•çš„API

1ã€ç”¨äºåˆ†ç±»çš„ä¼°è®¡å™¨ï¼š

* sklearn.neighbors k-è¿‘é‚»ç®—æ³•
* sklearn.naive_bayes è´å¶æ–¯
* sklearn.linear_model.LogisticRegression é€»è¾‘å›å½’
* sklearn.tree å†³ç­–æ ‘ä¸éšæœºæ£®æ—

2ã€ç”¨äºå›å½’çš„ä¼°è®¡å™¨ï¼š

* sklearn.linear_model.LinearRegression çº¿æ€§å›å½’
* sklearn.linear_model.Ridge å²­å›å½’

#### è¯„ä¼°æ ‡å‡†:ç²¾ç¡®ç‡(Precision)ä¸å¬å›ç‡(Recall)

ç²¾ç¡®ç‡ï¼šé¢„æµ‹ç»“æœä¸ºæ­£ä¾‹æ ·æœ¬ä¸­çœŸå®ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼ˆæŸ¥å¾—å‡†ï¼‰
![image](./images/3.png)

å¬å›ç‡ï¼šçœŸå®ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­é¢„æµ‹ç»“æœä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼ˆæŸ¥çš„å…¨ï¼Œå¯¹æ­£æ ·æœ¬çš„åŒºåˆ†èƒ½åŠ›ï¼‰
![image](./images/4.png)

$$F1 = \frac{2TP}{2TP + FN + FP} = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}$$

> æ¯”å¦‚: æ± å¡˜é‡Œæœ‰1400æ¡é±¼, 300åªè™¾, 300åªé³–. æœ€ç»ˆæ•è·äº†700æ¡é±¼,200åªè™¾, 300åªé³–. æˆ‘ä»¬è®¡ç®—é±¼æ•è·çš„æ¦‚ç‡:
> $æ­£ç¡®ç‡ = \frac{700}{700+200+100} = 0.7 $
> $å¬å›ç‡ = \frac{700}{1400} = 0.5$
> $Få€¼ = 0.7*0.5* \frac{2}{0.7+0.5} = 0.583$
> å› æ­¤å¯ä»¥çœ‹å‡º,å‡†ç¡®ç‡å’Œå¬å›ç‡å¾ˆå¤šæ—¶å€™æ˜¯å†²çªçš„, æ•´ä½“æ¦‚ç‡çš„ç²¾å‡†å°±ä¼šä¸¢å¤±å¬å›ç‡çš„è·å–. ä¸åŒåœºæ™¯éœ€è¦çš„æ˜¯ä¸åŒçš„æ ‡å‡†. ä½†æ˜¯å¤§éƒ¨åˆ†æ—¶å€™éœ€è¦åŒæ—¶è€ƒè™‘

åˆ†ç±»è¯„ä¼°çš„api`sklearn.metrics.classification_report`

`sklearn.metrics.classification_report(y_true,Â y_pred,Â target_names=None)`

* y_trueï¼šçœŸå®ç›®æ ‡å€¼
* y_predï¼šä¼°è®¡å™¨é¢„æµ‹ç›®æ ‡å€¼
* target_namesï¼šç›®æ ‡ç±»åˆ«åç§°
* returnï¼šæ¯ä¸ªç±»åˆ«ç²¾ç¡®ç‡ä¸å¬å›ç‡

```python
print("æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡\n", classification_report(y_test, y_predict, target_names=news.target_names))
# æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
#                            precision    recall  f1-score   support
#
#              alt.atheism       0.89      0.77      0.83       201
#            comp.graphics       0.93      0.78      0.85       256
#  comp.os.ms-windows.misc       0.86      0.81      0.84       261
# comp.sys.ibm.pc.hardware       0.74      0.85      0.79       255
#    comp.sys.mac.hardware       0.88      0.86      0.87       231
# ...
```

##### äº¤å‰éªŒè¯, ç½‘æ ¼æœç´¢(è¶…å‚æ•°æœç´¢)

äº¤å‰éªŒè¯ï¼šå°†æ‹¿åˆ°çš„æ•°æ®ï¼Œåˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯é›†ã€‚ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼šå°†æ•°æ®åˆ†æˆ5ä»½ï¼Œå…¶ä¸­ä¸€ä»½ä½œä¸ºéªŒè¯é›†ã€‚ç„¶åç»è¿‡5æ¬¡(ç»„)çš„æµ‹è¯•ï¼Œæ¯æ¬¡éƒ½æ›´æ¢ä¸åŒçš„éªŒè¯é›†ã€‚å³å¾—åˆ°5ç»„æ¨¡å‹çš„ç»“æœï¼Œ**å–å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆç»“æœ**ã€‚ç”±äºåˆ†ä¸ºäº†5ç»„æ‰€ä»¥ç§°ä¸º5æŠ˜äº¤å‰éªŒè¯, ä¹Ÿå¯ä»¥ä½¿ç”¨4æŠ˜äº¤å‰éªŒè¯ã€‚
![image](./images/5.png)

ç½‘æ ¼æœç´¢: é€šå¸¸æƒ…å†µä¸‹ï¼Œæœ‰å¾ˆå¤šå‚æ•°æ˜¯éœ€è¦æ‰‹åŠ¨æŒ‡å®šçš„ï¼ˆå¦‚k-è¿‘é‚»ç®—æ³•ä¸­çš„Kå€¼ï¼‰ï¼Œè¿™ç§å«è¶…å‚æ•°ã€‚ä½†æ˜¯æ‰‹åŠ¨è¿‡ç¨‹ç¹æ‚ï¼Œæ‰€ä»¥éœ€è¦å¯¹æ¨¡å‹é¢„è®¾å‡ ç§è¶…å‚æ•°ç»„åˆã€‚**æ¯ç»„è¶…å‚æ•°éƒ½é‡‡ç”¨äº¤å‰éªŒè¯æ¥è¿›è¡Œè¯„ä¼°**ã€‚æœ€åé€‰å‡ºæœ€ä¼˜å‚æ•°ç»„åˆå»ºç«‹æ¨¡å‹ã€‚æœ€å¸¸ç”¨çš„äº¤å‰éªŒè¯ä¸º10æŠ˜äº¤å‰éªŒè¯.
![image](./images/6.png)

api: `sklearn.model_selection.GridSearchCV`

`sklearn.model_selection.GridSearchCV(estimator,Â param_grid=None,cv=None)`å¯¹ä¼°è®¡å™¨çš„æŒ‡å®šå‚æ•°å€¼è¿›è¡Œè¯¦å°½æœç´¢

* estimatorï¼šä¼°è®¡å™¨å¯¹è±¡
* param_gridï¼šä¼°è®¡å™¨å‚æ•°(dict){â€œn_neighborsâ€:[1,3,5]}
* cvï¼šæŒ‡å®šå‡ æŠ˜äº¤å‰éªŒè¯
* fitï¼šè¾“å…¥è®­ç»ƒæ•°æ®
* scoreï¼šå‡†ç¡®ç‡

ç»“æœåˆ†æï¼š

* best_score_:åœ¨äº¤å‰éªŒè¯ä¸­æµ‹è¯•çš„æœ€å¥½ç»“æœ
* best_estimator_ï¼šæœ€å¥½çš„å‚æ•°æ¨¡å‹
* cv_results_:æ¯æ¬¡äº¤å‰éªŒè¯åçš„æµ‹è¯•é›†å‡†ç¡®ç‡ç»“æœå’Œè®­ç»ƒé›†å‡†ç¡®ç‡ç»“æœ

```python
# ä½¿ç”¨ç½‘æ ¼æœç´¢, éœ€è¦æ³¨æ„çš„æ˜¯ä¸éœ€è¦ç»™å‚æ•°å¦åˆ™å‚æ•°ä¼šå›ºå®š
knn = KNeighborsClassifier()

# æ„é€ å‚æ•°çš„å€¼è¿›è¡Œæœç´¢
param = {"n_neighbors": [3, 5, 10]}

# è¿›è¡Œç½‘æ ¼æœç´¢
gc = GridSearchCV(knn, param_grid=param, cv=2)
gc.fit(x_train, y_train)

# é¢„æµ‹å‡†ç¡®ç‡
print("åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡:", gc.score(x_test, y_test))
print("åœ¨äº¤å‰éªŒè¯å½“ä¸­æœ€å¥½çš„ç»“æœ:", gc.best_score_)
print("æœ€å¥½çš„æ¨¡å‹(å‚æ•°):", gc.best_estimator_)
print("æ¯ä¸ªè¶…å‚æ•°æ¯æ¬¡äº¤å‰éªŒè¯çš„ç»“æœ", gc.cv_results_)
# åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡: 0.4739952718676123
# åœ¨äº¤å‰éªŒè¯å½“ä¸­æœ€å¥½çš„ç»“æœ: 0.44774590163934425
# æœ€å¥½çš„æ¨¡å‹(å‚æ•°): KNeighborsClassifier(n_neighbors=10)
# æ¯ä¸ªè¶…å‚æ•°æ¯æ¬¡äº¤å‰éªŒè¯çš„ç»“æœ ...
```

### åˆ†ç±»

åˆ†ç±»æ˜¯ä¸€ç§åŸºäºä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡ç¡®å®šå› å˜é‡æ‰€å±ç±»åˆ«çš„æŠ€æœ¯ã€‚

#### kè¿‘é‚»ç®—æ³•

å®šä¹‰ï¼šå¦‚æœä¸€ä¸ªæ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„kä¸ªæœ€ç›¸ä¼¼(å³ç‰¹å¾ç©ºé—´ä¸­æœ€é‚»è¿‘)çš„æ ·æœ¬ä¸­çš„å¤§å¤šæ•°å±äºæŸä¸€ä¸ªç±»åˆ«ï¼Œåˆ™è¯¥æ ·æœ¬ä¹Ÿå±äºè¿™ä¸ªç±»åˆ«ã€‚

> *æ¥æºï¼šKNNç®—æ³•æœ€æ—©æ˜¯ç”±Coverå’ŒHartæå‡ºçš„ä¸€ç§åˆ†ç±»ç®—æ³•*

ä¸¤ä¸ªæ ·æœ¬çš„è·ç¦»å¯ä»¥é€šè¿‡å¦‚ä¸‹å…¬å¼è®¡ç®—ï¼Œåˆå«æ¬§å¼è·ç¦».ç›¸ä¼¼çš„æ ·æœ¬ç‰¹å¾ä¹‹é—´çš„è·ç¦»ä¼šå¾ˆè¿‘.ä¸å…¶è·ç¦»æœ€è¿‘çš„ç‚¹æœ€ç›¸ä¼¼. æ¯”å¦‚è¯´ï¼Œa(a1,a2,a3),b(b1,b2,b3), é‚£ä¹ˆè·ç¦»å°±æ˜¯:

$$\sqrt{((ğ‘1âˆ’ğ‘1)^2+(ğ‘2âˆ’ğ‘2)^2+(ğ‘3âˆ’ğ‘3)^2)}$$

å› æ­¤, **kè¿‘é‚»ç®—æ³•ä¼šéœ€è¦åšæ ‡å‡†åŒ–å¤„ç†**

Api: `sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')`

* n_neighborsï¼šint,å¯é€‰ï¼ˆé»˜è®¤= 5ï¼‰ï¼Œk_neighborsæŸ¥è¯¢é»˜è®¤ä½¿ç”¨çš„é‚»å±…æ•°

* algorithmï¼š{â€˜autoâ€™ï¼Œâ€˜ball_treeâ€™ï¼Œâ€˜kd_treeâ€™ï¼Œâ€˜bruteâ€™}ï¼Œå¯é€‰ç”¨äºè®¡ç®—æœ€è¿‘é‚»å±…çš„ç®—æ³•ï¼šâ€˜ball_treeâ€™å°†ä¼šä½¿ç”¨ BallTreeï¼Œâ€˜kd_treeâ€™å°†ä½¿ç”¨ KDTreeã€‚â€˜autoâ€™å°†å°è¯•æ ¹æ®ä¼ é€’ç»™fitæ–¹æ³•çš„å€¼æ¥å†³å®šæœ€åˆé€‚çš„ç®—æ³•ã€‚ (ä¸åŒå®ç°æ–¹å¼å½±å“æ•ˆç‡)

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler


def knn_alg():
    """
    kè¿‘é‚»ç®—æ³•
    k-nearest neighbors algorithm
    """
    # è¯»å–æ•°æ®
    data = pd.read_csv("E:\\Workspace\\ml\\machine-learning-python\\data\\FBlocation\\train.csv")

    # ç¼©å°æ•°æ®, é€šè¿‡queryæŸ¥è¯¢æ•°æ®
    data = data.query("x > 1.0 & x < 1.25 & y > 2.5 & y < 2.75")

    # å¯¹æ—¶é—´è¿›è¡Œå¤„ç†
    time_value = pd.to_datetime(data['time'], unit='s')

    # æŠŠæ—¥æœŸæ ¼å¼è½¬æ¢æˆå­—å…¸æ ¼å¼
    time_value = pd.DatetimeIndex(time_value)
    print(time_value)
    # DatetimeIndex(['1970-01-01 18:09:40', '1970-01-10 02:11:10',
    #                '1970-01-05 15:08:02', '1970-01-06 23:03:03',
    #                '1970-01-09 11:26:50', '1970-01-02 16:25:07',
    #                ...

    # æ·»åŠ feature
    data['day'] = time_value.day
    data['hour'] = time_value.hour
    data['weekday'] = time_value.weekday
    # data.loc[:, 'day'] = time_value.day
    # data.loc[:, 'hour'] = time_value.hour
    # data.loc[:, 'weekday'] = time_value.weekday
    print(data)
    #             row_id       x       y  accuracy    time    place_id  day  hour  weekday
    # 600            600  1.2214  2.7023        17   65380  6683426742    1    18        3
    # 957            957  1.1832  2.6891        58  785470  6683426742   10     2        5

    # ä»dataä¸­åˆ é™¤æ—¶é—´ç‰¹å¾, 1è¡¨ç¤ºåˆ—, 0è¡¨ç¤ºè¡Œ
    data = data.drop(['time'], axis=1)
    print(data)
    #             row_id       x       y  accuracy    place_id  day  hour  weekday
    # 600            600  1.2214  2.7023        17  6683426742    1    18        3
    # 957            957  1.1832  2.6891        58  6683426742   10     2        5

    # æŠŠç­¾åˆ°æ•°é‡å°‘äºnä¸ªçš„ä½ç½®åˆ é™¤
    place_count = data.groupby('place_id').count()

    # place_count.row_idå°±æˆäº†countçš„è¿”å›å€¼äº†, ç„¶åæŠŠå¤§äº3çš„indexä¿ç•™ä½, ä¹Ÿå°±æ˜¯è¿‡æ»¤æ‰äº†å°äº3çš„id, ä¹Ÿå°±æ˜¯count
    # ç„¶åreset_index()å°±æ˜¯æŠŠindexå˜ä¸ºä¸€ä¸ªåˆ—,æ­¤å¤„å°±æ˜¯place_id,ä¹Ÿå°±æ˜¯åˆšåˆšçš„groupbyçš„åç§°è®¾ç½®ä¸ºä¸€ä¸ªåˆ—
    tf = place_count[place_count.row_id > 3].reset_index()

    # dataä¸­çš„place_idæ˜¯å¦åœ¨tf.place_idä¸­ä¹Ÿå°±æ˜¯åœ¨dataä¸­åˆ é™¤å°äº3çš„ç‰¹å¾å€¼
    data = data[data['place_id'].isin(tf.place_id)]

    # å–å‡ºæ•°æ®å½“ä¸­çš„ç›®æ ‡å€¼å’Œç‰¹å¾å€¼
    y = data['place_id']
    x = data.drop(['place_id'], axis=1)
    x = x.drop(['row_id'], axis=1)

    print(x)

    # åˆ†å‰²æ•°æ®
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

    # ç‰¹å¾å·¥ç¨‹(standarlize)
    std = StandardScaler()
    # å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç‰¹å¾å€¼è¿›è¡Œæ ‡å‡†åŒ–
    x_train = std.fit_transform(x_train)
    x_test = std.fit_transform(x_test)

    # ç®—æ³•, è®¡ç®—æœ€è¿‘çš„5ä¸ªç‚¹
    knn = KNeighborsClassifier(n_neighbors=5)

    knn.fit(x_train, y_train)

    # å¾—å‡ºé¢„æµ‹ç»“æœ
    y_predict = knn.predict(x_test)

    print("é¢„æµ‹çš„ç›®æ ‡ç­¾åˆ°ä½ç½®: ", y_predict)

    # å‡†ç¡®ç‡
    print("é¢„æµ‹çš„å‡†ç¡®ç‡", knn.score(x_test, y_test))

    return None


if __name__ == '__main__':
    knn_alg()

```

ä¼˜ç¼ºç‚¹:

* ç®€å•ï¼Œæ˜“äºç†è§£ï¼Œæ˜“äºå®ç°ï¼Œæ— éœ€ä¼°è®¡å‚æ•°ï¼Œæ— éœ€è®­ç»ƒ

* kå€¼å–å¾ˆå°ï¼šå®¹æ˜“å—å¼‚å¸¸ç‚¹å½±å“
* kå€¼å–å¾ˆå¤§ï¼šå®¹æ˜“å—æœ€è¿‘æ•°æ®å¤ªå¤šå¯¼è‡´æ¯”ä¾‹å˜åŒ–
* æ—¶é—´å¤æ‚åº¦å¾ˆé«˜, æ€§èƒ½å¾ˆå·®, æ‡’æƒ°ç®—æ³•ï¼Œå¯¹æµ‹è¯•æ ·æœ¬åˆ†ç±»æ—¶çš„è®¡ç®—é‡å¤§ï¼Œå†…å­˜å¼€é”€å¤§
* å¿…é¡»æŒ‡å®šKå€¼ï¼ŒKå€¼é€‰æ‹©ä¸å½“åˆ™åˆ†ç±»ç²¾åº¦ä¸èƒ½ä¿è¯

#### æœ´ç´ è´å¶æ–¯ç®—æ³•

æ¦‚ç‡å®šä¹‰ä¸ºä¸€ä»¶äº‹æƒ…å‘ç”Ÿçš„å¯èƒ½æ€§.

ç‰¹ç‚¹: æ²¡æœ‰å‚æ•°, ä¸éœ€è¦è°ƒæ•´å‚æ•°

ä¼˜ç‚¹ï¼š

* æœ´ç´ è´å¶æ–¯æ¨¡å‹å‘æºäºå¤å…¸æ•°å­¦ç†è®ºï¼Œæœ‰ç¨³å®šçš„åˆ†ç±»æ•ˆç‡ã€‚
* å¯¹ç¼ºå¤±æ•°æ®ä¸å¤ªæ•æ„Ÿï¼Œç®—æ³•ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œå¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚
* åˆ†ç±»å‡†ç¡®åº¦é«˜ï¼Œé€Ÿåº¦å¿«

ç¼ºç‚¹ï¼šéœ€è¦çŸ¥é“å…ˆéªŒæ¦‚ç‡P(F1,F2,â€¦|C)ï¼Œå› æ­¤åœ¨æŸäº›æ—¶å€™ä¼šç”±äºå‡è®¾çš„å…ˆéªŒæ¨¡å‹çš„åŸå› å¯¼è‡´é¢„æµ‹æ•ˆæœä¸ä½³ã€‚

##### æ¦‚ç‡ç†è®º

è”åˆæ¦‚ç‡ï¼šåŒ…å«å¤šä¸ªæ¡ä»¶ï¼Œä¸”æ‰€æœ‰æ¡ä»¶åŒæ—¶æˆç«‹çš„æ¦‚ç‡, è®°ä½œï¼š$ğ‘ƒ(ğ´,ğµ)$, ä¹Ÿå°±æ˜¯ $P(A,B) = P(A) * P(B)$

æ¡ä»¶æ¦‚ç‡ï¼šå°±æ˜¯äº‹ä»¶Aåœ¨å¦å¤–ä¸€ä¸ªäº‹ä»¶Bå·²ç»å‘ç”Ÿæ¡ä»¶ä¸‹çš„å‘ç”Ÿæ¦‚ç‡, è®°ä½œï¼š$ğ‘ƒ(ğ´|ğµ)$ ä¹Ÿå°±æ˜¯Bçš„æ¡ä»¶ä¸‹Açš„æ¦‚ç‡

Bå­˜åœ¨çš„æƒ…å†µä¸‹å‘ç”Ÿ$A_1$å’Œ$A_2$çš„æ¦‚ç‡ï¼š$P(A_1,A_2|B) = P(A_1|B)P(A_2|B)$
æ³¨æ„ï¼šæ­¤æ¡ä»¶æ¦‚ç‡çš„æˆç«‹çš„å‰ææ˜¯ç”±äºA1,A2ç›¸äº’ç‹¬ç«‹çš„ç»“æœ, ä¸å­˜åœ¨ç›¸äº’å½±å“

è´å¶æ–¯å…¬å¼
$${\displaystyle P(C\mid W)={\frac {P(C)P(W\mid C)}{P(W)}}}$$

> Wä¸ºç»™å®šæ–‡æ¡£çš„ç‰¹å¾å€¼(é¢‘æ•°ç»Ÿè®¡, é¢„æµ‹çš„æ–‡æ¡£), Cä¸ºæ–‡æ¡£ç±»åˆ«
> æ‰€ä»¥å…¬å¼å¯ä»¥ç†è§£ä¸º: è¯¥ç±»æ–‡ç« æ€»ä½“å‡ºç°çš„æ¦‚ç‡ * æ¯ä¸€ä¸ªè¯åœ¨è¯¥ç±»æ–‡ç« ä¸­å‡ºç°çš„æ¦‚ç‡ / æ¯ä¸€ä¸ªè¯åœ¨æ‰€æœ‰æ–‡ç« ä¸­å‡ºç°çš„æ¦‚ç‡

å…¬å¼å¯ä»¥å†™ä½œ

$$ P(C \mid F_1,F_2, ...)={\frac {P(F_1,F_2,... \mid C)P(C)}{P(F_1,F_2,...)}}$$

> **æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘**
> ç”±äºç»™å®šçš„æ•°å€¼å¯èƒ½ä¸º0, ä¸€æ—¦å‡ºç°, æ‰€æœ‰çš„æ¦‚ç‡ç»“æœè®¡ç®—éƒ½ä¼šä¸º0. æ‰€ä»¥å¯ä»¥ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°æ”¾å…¥è®¡ç®—ä¸­
> $${\hat {\theta }}_{i}ä¹Ÿå°±æ˜¯P(F_1 | C)={\frac {x_{i}+\alpha }{N+\alpha m}}\qquad (i=1,\ldots ,m),$$
> æ­¤å¤„çš„$\alpha$ä¸ºæŒ‡å®šçš„ç³»æ•°, ä¸€èˆ¬ä¸º1, $m$ä¸ºè®­ç»ƒæ–‡æ¡£ä¸­ç»Ÿè®¡å‡ºçš„ç‰¹å¾è¯ä¸ªæ•°
> æ­¤æ—¶çš„å‡½æ•°å°±å˜ä¸ºäº† æ–‡ç« ç‰¹å¾è¯æ•° + $\alpha$/æ–‡ç« è¯æ•° + $(\alpha * ç‰¹å¾è¯ç§ç±»)$ X (å…¶ä»–çš„ç‰¹å¾è®¡ç®—)...

##### ä»£ç å®ç°

æœ´ç´ è´å¶æ–¯api: `sklearn.naive_bayes.MultinomialNB(alpha = 1.0)` æ­¤å¤„çš„1.0å°±æ˜¯æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°. é»˜è®¤1.0

```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB


def bayes_algorithom():
    """
    æœ´ç´ è´å¶æ–¯åˆ†ç±»ç®—æ³•
    :return:
    """
    # ä¸‹è½½æ–°é—»æ•°æ®
    news = fetch_20newsgroups(subset='all')

    # è¿›è¡Œæ•°æ®åˆ†éš”, 25%çš„æµ‹è¯•æ•°æ®
    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25)

    # å¯¹æ•°æ®åŠè¿›è¡Œç‰¹å¾æŠ½å–
    tf = TfidfVectorizer()  # ä½¿ç”¨æ–‡æœ¬ç‰¹å¾æŠ½å–
    x_train = tf.fit_transform(x_train)  # é’ˆå¯¹æ¯ç¯‡æ–‡ç« çš„è¯è¿›è¡Œç»Ÿè®¡
    x_test = tf.transform(x_test)  # ä½¿ç”¨åŒæ ·çš„ç‰¹å¾æŠ½å–æµ‹è¯•é›†, å¹¶è¿›è¡Œç»Ÿè®¡, è¿™æ ·ç‰¹å¾æ•°é‡æ˜¯ç›¸åŒçš„

    print(tf.get_feature_names_out())

    # è¿›è¡Œæœ´ç´ è´å¶æ–¯ç®—æ³•è¿›è¡Œé¢„æµ‹
    mlt = MultinomialNB(alpha=1.0)
    print(x_train.toarray())
    mlt.fit(x_train, y_train)

    # å¾—å‡ºå‡†ç¡®ç‡
    y_predict = mlt.predict(x_test)
    print("é¢„æµ‹çš„æ–‡ç« ç±»åˆ«ä¸º: ", y_predict)
    print("å‡†ç¡®ç‡ä¸º: ", mlt.score(x_test, y_test))
    # é¢„æµ‹çš„æ–‡ç« ç±»åˆ«ä¸º:  [13 10  7 ... 15 15 10]
    # å‡†ç¡®ç‡ä¸º:  0.8552631578947368

    return None


if __name__ == '__main__':
    bayes_algorithom()
```

#### å†³ç­–æ ‘

å†³ç­–æ ‘æ€æƒ³çš„æ¥æºéå¸¸æœ´ç´ ï¼Œç¨‹åºè®¾è®¡ä¸­çš„æ¡ä»¶åˆ†æ”¯ç»“æ„å°±æ˜¯if-thenç»“æ„ï¼Œæœ€æ—©çš„å†³ç­–æ ‘å°±æ˜¯åˆ©ç”¨è¿™ç±»ç»“æ„åˆ†å‰²æ•°æ®çš„ä¸€ç§åˆ†ç±»å­¦ä¹ æ–¹æ³•.

![å†³ç­–æ ‘](./images/8.png)

åœ¨å†³ç­–æ ‘ä¸­æ¯ä¸€ä¸ªèŠ‚ç‚¹çš„ä½ç½®å°±æ˜¯ä¼˜å…ˆçº§, é«˜ä¼˜å…ˆçº§çš„èŠ‚ç‚¹ä¼šè¢«ä¼˜å…ˆè®¡ç®—, ç„¶åæ‰ä¼šè¿›å…¥ä¸‹ä¸€ä¸ªåˆ†ç±»èŠ‚ç‚¹

> åœ¨ä¿¡æ¯è®ºä¸­, ä½¿ç”¨binary searchå¯ä»¥è·å–ä¿¡æ¯å•ä½. æ¯”å¦‚:
> 32ä¸ªå°çƒä¸­æœç´¢ï¼Œlog32=5æ¯”ç‰¹
> 64ä¸ªå°çƒä¸­æœç´¢ï¼Œlog64=6æ¯”ç‰¹

ä»è€Œè®¡ç®—ä¿¡æ¯ç†µä¸º $H = -(P_1logP_1 + P_2logP_2 + ... + P_{32}logP_{32})$, **ä¿¡æ¯å’Œæ¶ˆé™¤ä¸ç¡®å®šæ˜¯ç›¸è”ç³»çš„**. æ•°å­¦å…¬å¼ä¹Ÿå°±æ˜¯
$$ H(x) = -{\sum}_{x \in X} P(x) log P(x) $$

ä¿¡æ¯å¢ç›Š: å½“å¾—çŸ¥ä¸€ä¸ªç‰¹å¾æ¡ä»¶, å‡å°‘çš„ä¿¡æ¯ç†µçš„å¤§å°: $g(D,A) = H(D) - H(D|A)$

ç‰¹å¾Aå¯¹è®­ç»ƒé›†æ•°æ®Dçš„ä¿¡æ¯å¢ç›Š$g(D,A)$ ç­‰äºé›†åˆDçš„ä¿¡æ¯ç†µ$H(D)$å‡å»ç‰¹å¾Aç»™å®šæ¡ä»¶ä¸‹ä¿¡æ¯æ¡ä»¶ç†µ$H(D|A)$.

ä¿¡æ¯ç†µçš„è®¡ç®—å…¬å¼:
$$H(D)=-\sum^{K}_{k=1}\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}$$

æ¡ä»¶ç†µçš„è®¡ç®—å…¬å¼:
$$H(D|A)=-\sum^{n}_{i=1}\frac{|D_i|}{|D|}H(D_i)=-\sum^{n}_{i=1}\frac{|D_i|}{|D|}\sum^{K}_{k=1}\frac{|D_{ik}|}{|D_i|}log\frac{|D_{ik}|}{|D_i|} $$

ä¸¾ä¾‹:
![7.png](images/7.png)

1. é’ˆå¯¹ç‰¹å¾`ç±»åˆ«`è®¡ç®—ä¿¡æ¯ç†µ. æ­¤å¤„çš„ç±»åˆ«æ˜¯æœ€ç»ˆåˆ†ç±»ç»“æœ.

    ä¸€å…±æœ‰15ä¸ªç‰¹å¾æ€»æ•°, 9ä¸ª`æ˜¯`6ä¸ª`å¦`
    $H(ç±»åˆ«) = -(\frac{9}{15} log\frac{9}{15} + \frac{6}{15} log\frac{6}{15}) = 0.971$

2. åœ¨ç±»åˆ«`ç‰¹å¾`ä¿¡æ¯ç†µç»“æœä¸‹, è®¡ç®—`å¹´é¾„`ç‰¹å¾çš„ä¿¡æ¯å¢ç›Š,

    $g(D,å¹´é¾„) = H(D) - H(D'|å¹´é¾„)$

    * å¹´é¾„æ€»å…±å­˜åœ¨3å„ç±»åˆ«, é’å¹´/ä¸­å¹´/è€å¹´, éœ€è¦åˆ†åˆ«æ±‚å‡ºç„¶åæ±‚å’Œ
    * H(D) å°±æ˜¯æˆ‘ä»¬ä¹‹å‰æ±‚å‡ºæ¥çš„`ç±»åˆ«`çš„ä¿¡æ¯ç†µ, è®¡ç®—å‡ºæ¥ç»“æœå°±æ˜¯0.971
    * é’å¹´ä¸­å¹´å’Œè€å¹´çš„å æ¯”éƒ½ä¸º$\frac{5}{15}$ä¹Ÿå°±æ˜¯$\frac{1}{3}$
        $$g(D,å¹´é¾„) = 0.971 - [\frac{1}{3}(é’å¹´) + \frac{1}{3}(ä¸­å¹´) + \frac{1}{3}(è€å¹´)]$$
    * H(é’å¹´) = å½“å¹´é¾„å…¨éƒ½æ˜¯`é’å¹´`çš„æ—¶å€™, `ç±»åˆ«`çš„ä¿¡æ¯ç†µ. å½“å¹´é¾„ä¸º`é’å¹´`çš„æ—¶å€™ç±»åˆ«3ä¸ª`å¦`1ä¸ª`æ˜¯`
        $$H(é’å¹´) = -(\frac{2}{5}log\frac{2}{5} + \frac{3}{5}log\frac{3}{5})$$
        $$H(ä¸­å¹´) = -(\frac{2}{5}log\frac{2}{5} + \frac{3}{5}log\frac{3}{5})$$
        $$H(è€å¹´) = -(\frac{4}{5}log\frac{4}{5} + \frac{1}{5}log\frac{1}{5})$$

3. é€šè¿‡è®¡ç®—æˆ‘ä»¬å‘ç°, g(ç±»åˆ«,å·¥ä½œ),g(ç±»åˆ«,æˆ¿å­),g(ç±»åˆ«,ä¿¡è´·)çš„è®¡ç®—ç»“æœæœ€ç»ˆä¸º:`0.324`,`0.420`,`0.363`. å› æ­¤`æœ‰æˆ¿å­`çš„ä¿¡æ¯å¢ç›Šè®¡ç®—ç»“æœä¸ºæœ€å¤§. å› æ­¤`æœ‰æˆ¿å­`å°±ä¸ºæ ‘çš„æ ¹èŠ‚ç‚¹, ä¹Ÿå°±æ˜¯ç¬¬ä¸€ä¸ªåˆ¤æ–­èŠ‚ç‚¹

>
> å†³ç­–æ ‘åŒæ—¶è¿˜å­˜åœ¨å…¶ä»–çš„è®¡ç®—
>> id3: ä¿¡æ¯å¢ç›Š, æœ€å¤§å‡†åˆ™, ä¹Ÿå°±æ˜¯ä¸Šé¢çš„è¿ç®—ç­–ç•¥
>> c4.5: ä¿¡æ¯å¢ç›Šæ¯”é‡, æœ€å¤§å‡†åˆ™
>> cart:
>>> å›å½’æ ‘: å¹³æ–¹è¯¯å·®å–æœ€å°
>>> åˆ†ç±»æ ‘: åŸºå°¼ç³»æ•° å–æœ€å°, åœ¨sklearnä¸­å¯ä»¥é€‰æ‹©åˆ’åˆ†çš„åŸåˆ™. åˆ’åˆ†æ›´åŠ ä»”ç»†

å†³ç­–æ ‘api:

```python
classÂ sklearn.tree.DecisionTreeClassifier(criterion=â€™giniâ€™,Â max_depth=None,random_state=None)
```

åˆ†ç±»å™¨:

* criterion:é»˜è®¤æ˜¯`gini`ç³»æ•°ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä¿¡æ¯å¢ç›Šçš„ç†µ`entropy`
* max_depth:æ ‘çš„æ·±åº¦å¤§å°
* random_state:éšæœºæ•°ç§å­

method:

* decision_path:è¿”å›å†³ç­–æ ‘çš„è·¯å¾„

```python
# å†³ç­–æ ‘
# å†³ç­–æ ‘å­˜åœ¨ä¸€ä¸ªå‡çº§ç‰ˆå«åšéšæœºæ£®æ—
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_graphviz


def discussion_tree_alg():
    """
    æ³°å¦å°¼å…‹å·çš„æ•°æ®åˆ†ç±»
    https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic.txt

    1: å­˜æ´»
    0: æ­»äº¡
    """
    print("å†³ç­–æ ‘é€‰æ‹©è¿‡ç¨‹")
    # è¯»å–æ•°æ®
    data = pd.read_csv("E:\\Workspace\\ml\\machine-learning-python\\data\\titanic.txt")
    print(data)

    # å¤„ç†æ•°æ®, æ‰¾å‡ºç‰¹å¾å€¼å’Œç›®æ ‡å€¼
    x = data[['pclass', 'age', 'sex']]  # ç‰¹å¾å€¼çš„åˆ—
    print(x)
    y = data['survived']  # ç»“æœé›†

    # å¤„ç†ç¼ºå¤±å€¼ inplaceè¡¨ç¤ºæ›¿æ¢. æŠŠå¹³å‡å€¼å¡«å…¥ageä¸­
    x['age'].fillna(x['age'].mean(), inplace=True)

    # åˆ†å‰²æ•°æ®é›†, è®­ç»ƒé›†, æµ‹è¯•é›†
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

    # ç‰¹å¾å·¥ç¨‹å¤„ç†
    dict = DictVectorizer(sparse=False)
    # to_dict() å°±æ˜¯å°†æ ·æœ¬è½¬æ¢ä¸ºå­—å…¸ç±»å‹, orient="records"è¡¨ç¤ºæ¯ä¸€è¡Œä¸ºä¸€ä¸ªå­—å…¸
    x_train = dict.fit_transform(x_train.to_dict(orient="records"))
    x_test = dict.fit_transform(x_test.to_dict(orient="records"))
    print(dict.get_feature_names())
    print(x_train)
    print(x_test)
    # ['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']
    # [[31.19418104  0.          0.          1.          1.          0.        ]
    #  [39.          1.          0.          0.          1.          0.        ]
    #  [ 1.          0.          1.          0.          1.          0.        ]
    #  ...
    #  [18.          0.          1.          0.          1.          0.        ]
    #  [45.          0.          1.          0.          1.          0.        ]
    #  [ 9.          0.          0.          1.          1.          0.        ]]

    # ç”¨å†³ç­–æ ‘è¿›è¡Œé¢„æµ‹
    dec = DecisionTreeClassifier()
    # dec = DecisionTreeClassifier(max_depth=5)  # æ·±åº¦ä¸º5
    dec.fit(x_train, y_train)

    print("é¢„æµ‹çš„å‡†ç¡®ç‡:")
    print(dec.score(x_test, y_test))

    return None


if __name__ == '__main__':
    discussion_tree_alg()

```

1. å±•ç¤ºå†³ç­–æ ‘çš„è·¯å¾„å›¾api `sklearn.tree.export_graphviz()`Â è¯¥å‡½æ•°èƒ½å¤Ÿå¯¼å‡ºDOTæ ¼å¼, apiä¸º: `tree.export_graphviz(estimator,out_file='tree.dotâ€™,feature_names=[â€˜â€™,â€™â€™])`

```python
export_graphviz(dec, out_file="./tree.dot",
                feature_names=['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male'])
```

2. å¯¼å‡ºå·¥å…·:(èƒ½å¤Ÿå°†dotæ–‡ä»¶è½¬æ¢ä¸ºpdfã€png), å®‰è£…graphviz:
    ubuntu:sudo apt-get install graphviz
    Mac:brew install graphviz
    Windows: <https://graphviz.org/download/>

    åœ¨shellä¸­æ‰§è¡Œ`dot -Tpng tree.dot -o tree.png`
    ç„¶åå°±å¯ä»¥åœ¨ç›®å½•ä¸‹æ‰¾åˆ°tree.pngæ–‡ä»¶ ![9.png](images/9.png)

> éœ€è¦æ³¨æ„çš„æ˜¯, ä¸Šå›¾æ‰€ç¤ºçš„ä½¿ç”¨çš„å°±æ˜¯åŸºå°¼ç³»æ•°.ç”±äºåŸºå°¼ç³»æ•°æ˜¯å¼ºåˆ†ç±», åˆ›å»ºåºå¤§çš„åˆ†ç±»æ ‘, åŸºå°¼ç³»æ•°å¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆçš„é—®é¢˜. æ­¤ç§é—®é¢˜çš„è§£å†³æ–¹æ¡ˆå°±æ˜¯éšæœºæ£®æ—.

å†³ç­–æ ‘ä¼˜ç‚¹ï¼š

* ç®€å•çš„ç†è§£å’Œè§£é‡Šï¼Œæ ‘æœ¨å¯è§†åŒ–ã€‚
* éœ€è¦å¾ˆå°‘çš„æ•°æ®å‡†å¤‡ï¼Œå…¶ä»–æŠ€æœ¯é€šå¸¸éœ€è¦æ•°æ®å½’ä¸€åŒ–ï¼Œ

å†³ç­–æ ‘ç¼ºç‚¹ï¼š

* å†³ç­–æ ‘å­¦ä¹ è€…å¯ä»¥åˆ›å»ºä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿æ•°æ®çš„è¿‡äºå¤æ‚çš„æ ‘ï¼Œè¿™è¢«ç§°ä¸ºè¿‡æ‹Ÿåˆã€‚
* å†³ç­–æ ‘å¯èƒ½ä¸ç¨³å®šï¼Œå› ä¸ºæ•°æ®çš„å°å˜åŒ–å¯èƒ½ä¼šå¯¼è‡´å®Œå…¨ä¸åŒçš„æ ‘è¢«ç”Ÿæˆ

è§£å†³æ–¹æ¡ˆ:

* å‡æcartç®—æ³•. å½“æ ·æœ¬æ•°å°‘äºä¸€å®šæ•°ç›®çš„æ—¶å€™æˆ–è€…æ»¡è¶³ä¸€å®šæ¡ä»¶çš„æ—¶å€™ä¼šå¯¹æ ‘è¿›è¡Œå‰ªæ. ä¸»è¦é’ˆå¯¹å¶å­ç»“ç‚¹.
* éšæœºæ£®æ—

#### éšæœºæ£®æ—

é›†æˆå­¦ä¹ çš„æ–¹æ³•. é›†æˆå­¦ä¹ é€šè¿‡å»ºç«‹å‡ ä¸ªæ¨¡å‹ç»„åˆçš„æ¥è§£å†³å•ä¸€é¢„æµ‹é—®é¢˜ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯ç”Ÿæˆå¤šä¸ªåˆ†ç±»å™¨/æ¨¡å‹ï¼Œå„è‡ªç‹¬ç«‹åœ°å­¦ä¹ å’Œä½œå‡ºé¢„æµ‹ã€‚è¿™äº›é¢„æµ‹æœ€åç»“åˆæˆå•é¢„æµ‹ï¼Œå› æ­¤ä¼˜äºä»»ä½•ä¸€ä¸ªå•åˆ†ç±»çš„åšå‡ºé¢„æµ‹ã€‚

éšæœºæ£®æ—çš„ç»“æœå°±æ˜¯å¤šä¸ªå†³ç­–æ ‘çš„æŠ•ç¥¨. å»ºç«‹å¤šä¸ªå†³ç­–æ ‘çš„è¿‡ç¨‹:

å•ä¸ªæ ‘çš„å»ºç«‹è¿‡ç¨‹(nä¸ªæ ·æœ¬, mä¸ªç‰¹å¾):

1. éšæœºåœ¨nä¸ªæ ·æœ¬å½“ä¸­æŠ½å–ä¸€ä¸ªæ ·æœ¬, åœ¨æŠ½å–æ ·æœ¬çš„æ± å­ä¸å˜çš„æƒ…å†µä¸‹é‡å¤næ¬¡. å› æ­¤æ ·æœ¬é«˜æ¦‚ç‡å­˜åœ¨é‡å¤.
2. éšæœºåœ¨mä¸ªç‰¹å¾ä¸­è·å–$m_1$ä¸ªç‰¹å¾

å› æ­¤,æ¯ä¸ªå†³ç­–æ ‘çš„ç‰¹å¾å’Œæ ·æœ¬ä¸åŒ

éšæœºæ£®æ—api: `classÂ sklearn.ensemble.RandomForestClassifier(n_estimators=10,Â criterion=â€™giniâ€™,Â max_depth=None, bootstrap=True,Â random_state=None)` å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„åŒ…åŒ…å«çš„æ˜¯`ensemble`ç±»å‹, ä¹Ÿå°±æ˜¯é›†æˆå­¦ä¹ . (è¿™é‡Œè¿˜æ˜¯ä½¿ç”¨çš„æ˜¯åŸºå°¼ç³»æ•°, ä¹Ÿæ˜¯æ¨èçš„)

éšæœºæ£®æ—åˆ†ç±»å™¨:

* n_estimatorsï¼šintegerï¼Œoptionalï¼ˆdefault = 10ï¼‰ æ£®æ—é‡Œçš„æ ‘æœ¨æ•°é‡, å¯ä»¥ä½¿ç”¨120,200,300,500,800,1200
* criteriaï¼šstringï¼Œå¯é€‰ï¼ˆdefault =â€œginiâ€ï¼‰åˆ†å‰²ç‰¹å¾çš„æµ‹é‡æ–¹æ³•
* max_depthï¼šintegeræˆ–Noneï¼Œå¯é€‰ï¼ˆé»˜è®¤=æ— ï¼‰æ ‘çš„æœ€å¤§æ·±åº¦, æ¨è: 5,8,15,25,30
* max_features="auto" æ¯ä¸ªå†³ç­–æ ‘çš„æœ€å¤§ç‰¹å¾æ•°é‡
  * auto: max_features=sqrt(n_features)
  * sqrt: max_features=sqrt(n_features)
  * log2: max_features=log2(n_features)
  * none: max_features=n_features
* bootstrapï¼šbooleanï¼Œoptionalï¼ˆdefault = Trueï¼‰æ˜¯å¦åœ¨æ„å»ºæ ‘æ—¶ä½¿ç”¨æ”¾å›æŠ½æ ·

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV


def discussion_tree_alg():
    """
    æ³°å¦å°¼å…‹å·çš„æ•°æ®åˆ†ç±»
    https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic.txt

    1: å­˜æ´»
    0: æ­»äº¡
    """
    print("å†³ç­–æ ‘é€‰æ‹©è¿‡ç¨‹")
    # è¯»å–æ•°æ®
    data = pd.read_csv("E:\\Workspace\\ml\\machine-learning-python\\data\\titanic.txt")
    print(data)

    # å¤„ç†æ•°æ®, æ‰¾å‡ºç‰¹å¾å€¼å’Œç›®æ ‡å€¼
    x = data[['pclass', 'age', 'sex']]  # ç‰¹å¾å€¼çš„åˆ—
    print(x)
    y = data['survived']  # ç»“æœé›†

    # å¤„ç†ç¼ºå¤±å€¼ inplaceè¡¨ç¤ºæ›¿æ¢. æŠŠå¹³å‡å€¼å¡«å…¥ageä¸­
    x['age'].fillna(x['age'].mean(), inplace=True)

    # åˆ†å‰²æ•°æ®é›†, è®­ç»ƒé›†, æµ‹è¯•é›†
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

    # ç‰¹å¾å·¥ç¨‹å¤„ç†
    dict = DictVectorizer(sparse=False)
    # to_dict() å°±æ˜¯å°†æ ·æœ¬è½¬æ¢ä¸ºå­—å…¸ç±»å‹, orient="records"è¡¨ç¤ºæ¯ä¸€è¡Œä¸ºä¸€ä¸ªå­—å…¸
    x_train = dict.fit_transform(x_train.to_dict(orient="records"))
    x_test = dict.fit_transform(x_test.to_dict(orient="records"))
    print(dict.get_feature_names_out())
    print(x_train)
    print(x_test)
    # ['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']
    # [[31.19418104  0.          0.          1.          1.          0.        ]
    #  [39.          1.          0.          0.          1.          0.        ]
    #  [ 1.          0.          1.          0.          1.          0.        ]
    #  ...
    #  [18.          0.          1.          0.          1.          0.        ]
    #  [45.          0.          1.          0.          1.          0.        ]
    #  [ 9.          0.          0.          1.          1.          0.        ]]

    # ä½¿ç”¨éšæœºæ£®æ—
    # (n_estimators=10,Â criterion=â€™giniâ€™,Â max_depth=None, bootstrap=True,Â random_state=None)
    rfc = RandomForestClassifier()  # é»˜è®¤æ•°æ®

    # ç½‘æ ¼æœç´¢ä¸äº¤å‰éªŒè¯
    print("æ­£åœ¨è¿›è¡Œç½‘æ ¼å‚æ•°è°ƒä¼˜...");
    params = {"n_estimators": [120, 200, 300, 500, 800, 1200], "max_depth": [5, 8, 15, 25, 30]}
    gc = GridSearchCV(rfc, param_grid=params, cv=2)  # ç½‘æ ¼äº¤å‰éªŒè¯, é…ç½®äº¤å‰éªŒè¯ä¸º2

    gc.fit(x_train, y_train)

    print("é¢„æµ‹å‡†ç¡®ç‡")
    print(gc.score(x_test, y_test))
    print("é€‰æ‹©çš„å‚æ•°æ¨¡å‹:")
    print(gc.best_params_)

    # é¢„æµ‹å‡†ç¡®ç‡
    # 0.8297872340425532
    # é€‰æ‹©çš„å‚æ•°æ¨¡å‹:
    # {'max_depth': 5, 'n_estimators': 300}

    return None


if __name__ == '__main__':
    discussion_tree_alg()

```

éšæœºæ£®æ—ç‰¹ç‚¹

* åœ¨å½“å‰æ‰€æœ‰ç®—æ³•ä¸­ï¼Œå…·æœ‰æå¥½çš„å‡†ç¡®ç‡
* èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿è¡Œåœ¨å¤§æ•°æ®é›†ä¸Š
* èƒ½å¤Ÿå¤„ç†å…·æœ‰é«˜ç»´ç‰¹å¾çš„è¾“å…¥æ ·æœ¬ï¼Œè€Œä¸”ä¸éœ€è¦é™ç»´
* èƒ½å¤Ÿè¯„ä¼°å„ä¸ªç‰¹å¾åœ¨åˆ†ç±»é—®é¢˜ä¸Šçš„é‡è¦æ€§
* å¯¹äºç¼ºçœå€¼é—®é¢˜ä¹Ÿèƒ½å¤Ÿè·å¾—å¾ˆå¥½å¾—ç»“æœ

### å›å½’

åœ¨å¤§æ•°æ®åˆ†æä¸­ï¼Œå›å½’åˆ†ææ˜¯ä¸€ç§é¢„æµ‹æ€§çš„å»ºæ¨¡æŠ€æœ¯ï¼Œå®ƒç ”ç©¶çš„æ˜¯å› å˜é‡ï¼ˆç›®æ ‡ï¼‰å’Œè‡ªå˜é‡ï¼ˆé¢„æµ‹å™¨ï¼‰ä¹‹é—´çš„å…³ç³»ã€‚è¿™ç§æŠ€æœ¯é€šå¸¸ç”¨äºé¢„æµ‹åˆ†æï¼Œæ—¶é—´åºåˆ—æ¨¡å‹ä»¥åŠå‘ç°å˜é‡ä¹‹é—´çš„å› æœå…³ç³»ã€‚

#### çº¿æ€§å›å½’

æŸå¤±å‡½æ•°:

$$J(\theta) = (h_w(x_1) - y_1)^2 + (h_w(x_2) - y_2)^2 + ... + (h_w(x_m) - y_m)^2 = \sum_{i=1}^{m}(h_w(x_i) - y_i)^2$$

çº¿æ€§å›å½’çš„ç›®æ ‡å°±æ˜¯é€šè¿‡ä¸åœçš„è¿­ä»£æ‰¾åˆ°æŸå¤±å‡½æ•°æœ€ä½çš„å€¼

> æœ€å°äºŒä¹˜æ³• - *æ­£è§„æ–¹ç¨‹* çº¿æ€§å›å½’çš„å¦ä¸€ç§æ±‚è§£æ–¹æ³•.
>
> æ±‚è§£: $w = (X^TX)^{-1}X^Ty$ æ­¤å¤„Xä¸ºç‰¹å¾å€¼çš„çŸ©é˜µ, yä¸ºç›®æ ‡å€¼çš„çŸ©é˜µ.
>
> å°±æ˜¯"ç‰¹å¾çŸ©é˜µXçš„è½¬ç½®çŸ©é˜µ"ä¹˜ä»¥"ç‰¹è¯ŠçŸ©é˜µ"æœ¬èº«, å¯¹å…¶è¿›è¡Œé€†çŸ©é˜µæ“ä½œ, ç»“æœå†ä¹˜ä»¥"ç‰¹å¾çŸ©é˜µçš„è½¬ç½®çŸ©é˜µ", æœ€åå†ä¹˜ä»¥ç»“æœçŸ©é˜µy. ç»“æœå°†ä¼šæ˜¯ä¸€ä¸ªç‰¹å¾æœ¬èº«æ•°é‡çš„å•è¡ŒçŸ©é˜µ.
>
> *ä½†æ˜¯è¿™ç§æ–¹æ³•å¤æ‚åº¦ç›¸å½“é«˜. æ±‚è§£é€Ÿåº¦ä¹Ÿå¾ˆæ…¢.* å…¶ä¸»è¦åŸå› æ˜¯å¤§é‡çš„çŸ©é˜µè¿ç®—æ“ä½œ.

æ¢¯åº¦ä¸‹é™(**å¸¸ç”¨**)

è¿­ä»£å…¬å¼:
$$w_1 := -w_1 - \alpha\frac{\partial cost(w_0 + w_1 x_1)}{\partial w_1}$$
$$w_0 := -w_0 - \alpha\frac{\partial cost(w_0 + w_1 x_1)}{\partial w_1}$$

æ­¤å¤„$\alpha$ä¸ºå­¦ä¹ é€Ÿç‡, éœ€è¦æ‰‹åŠ¨æŒ‡å®š. $\frac{\partial cost(w_0 + w_1 x_1)}{\partial w_1}$ åˆ™æ˜¯å­¦ä¹ æ–¹å‘.

æ¢¯åº¦ä¸‹é™å…¬å¼:
$$w_j := w_j - \alpha\frac{1}{m}\sum^{m}_{i=1}(\sum^{n}_{j=0}w_jx^{(i)}_j-y^{(i)})x^{(i)}_j$$

çº¿æ€§å›å½’api:
`sklearn.linear_model.LinearRegression` æ­£è§„æ–¹ç¨‹, coef_ï¼šå›å½’ç³»æ•°
`sklearn.linear_model.SGDRegressor` æ¢¯åº¦ä¸‹é™, coef_ï¼šå›å½’ç³»æ•°

åŒºåˆ†æ­£è§„æ–¹ç¨‹å’Œæ¢¯åº¦ä¸‹é™çš„æ€§èƒ½æ˜¯ä¸åŒçš„, ä¸€èˆ¬æ•°æ®ä¸º100kä»¥ä¸Šå°±ä½¿ç”¨æ¢¯åº¦ä¸‹é™

ä½¿ç”¨æ³¢å£«é¡¿æ”¾å‡æ•°æ®æ¡ˆä¾‹ `from sklearn.datasets import load_boston`, æˆ–è€…ä»å®˜ç½‘ä¸‹è½½ <http://lib.stat.cmu.edu/datasets/boston> *éœ€è¦æ³¨æ„çš„æ˜¯, load_bostonæ•°æ®å¯èƒ½å­˜åœ¨ä¸å‡†ç¡®ç­‰é—®é¢˜, åœ¨æœªæ¥å¯èƒ½ä¼šè¢«ç§»é™¤. å› æ­¤å¯ä»¥ä½¿ç”¨ `housing = fetch_california_housing()`*

```python
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


def linear_regression():
    """
    çº¿æ€§å›å½’ç›´æ¥é¢„æµ‹æˆ¿å±‹ä»·æ ¼
    :return:
    """
    # è·å–æ•°æ®
    data = fetch_california_housing()
    print(data.feature_names)
    # ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']
    print(data.data)
    print(data.target)  # [4.526 3.585 3.521 ... 0.923 0.847 0.894]

    # åˆ†å‰²æ•°æ®é›†
    x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25)

    # æ ‡å‡†åŒ–
    # ç‰¹å¾å€¼éƒ½å¿…é¡»è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†, å®ä¾‹åŒ–ä¸¤ä¸ªæ ‡å‡†åŒ–api
    std_x = StandardScaler()
    x_train = std_x.fit_transform(x_train)
    x_test = std_x.fit_transform(x_test)

    # éœ€è¦æ³¨æ„çš„æ˜¯åœ¨0.19ç‰ˆæœ¬ä»¥å, æ ‡å‡†åŒ–åªèƒ½ä½œç”¨åœ¨å¤šç»´æ•°ç»„ä¸Š, ä¸èƒ½ä½œç”¨åœ¨ä¸€ç»´æ•°ç»„ä¸Š
    std_y = StandardScaler()
    # y_train = std_y.fit_transform(y_train)
    y_train = std_y.fit_transform(y_train.reshape(-1, 1))
    # y_test = std_y.fit_transform(y_test)
    y_test = std_y.fit_transform(y_test.reshape(-1, 1))

    # é¢„æµ‹
    # ==========================æ­£è§„æ–¹ç¨‹æ±‚è§£æ–¹å¼===========================
    lr = LinearRegression()
    lr.fit(x_train, y_train)
    print("æ¨¡å‹å‚æ•°: ", lr.coef_)  # è·å–å…¨éƒ¨çš„wå€¼, ä¹Ÿå°±æ˜¯å›å½’ç³»æ•°

    # æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
    y_predict = lr.predict(x_test)
    # ä½¿ç”¨åè½¬æ ‡å‡†åŒ–
    # print("æµ‹è¯•é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ:", std_y.inverse_transform(y_predict))

    # ===========================æ¢¯åº¦ä¸‹é™=================================
    sgd = SGDRegressor()
    sgd.fit(x_train, y_train)
    print("æ¨¡å‹å‚æ•°: ", sgd.coef_)
    y_sgd_predict = sgd.predict(x_test)
    print("æµ‹è¯•é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ:", y_sgd_predict)

    return None


if __name__ == '__main__':
    linear_regression()

```

å›å½’æ€§èƒ½çš„è¯„ä¼°: å‡æ–¹è¯¯å·®(Mean Squared Error)MSE
$$MSE = \frac{1}{m}\sum_{i=1}^{m}(y^i - \overline{y})^2$$

å‡æ–¹è¯¯å·®api: `sklearn.metrics.mean_squared_error(y_true,y_pred)`

å‡æ–¹è¯¯å·®å›å½’æŸå¤±, çœŸå®å€¼ï¼Œé¢„æµ‹å€¼ä¸ºæ ‡å‡†åŒ–ä¹‹å‰çš„å€¼

* y_true:çœŸå®å€¼
* y_pred:é¢„æµ‹å€¼
* return:æµ®ç‚¹æ•°ç»“æœ

```python
    # é¢„æµ‹
    # ==========================æ­£è§„æ–¹ç¨‹æ±‚è§£æ–¹å¼===========================
    lr = LinearRegression()
    lr.fit(x_train, y_train)
    print("æ¨¡å‹å‚æ•°: ", lr.coef_)  # è·å–å…¨éƒ¨çš„wå€¼, ä¹Ÿå°±æ˜¯å›å½’ç³»æ•°

    # æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
    y_predict = lr.predict(x_test)
    # ä½¿ç”¨åè½¬æ ‡å‡†åŒ–
    print("æµ‹è¯•é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ:", std_y.inverse_transform(y_predict))
    print("å‡æ–¹è¯¯å·®", mean_squared_error(y_test, std_y.inverse_transform(y_predict)))
    # å‡æ–¹è¯¯å·® 0.5281526459634236

    # ===========================æ¢¯åº¦ä¸‹é™=================================
    sgd = SGDRegressor()
    sgd.fit(x_train, y_train)
    print("æ¨¡å‹å‚æ•°: ", sgd.coef_)
    y_sgd_predict = sgd.predict(x_test).reshape(-1, 1)  # sgd è¿”å›ä¸€ç»´æ•°ç»„
    print("æµ‹è¯•é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ:", y_sgd_predict)
    print("å‡æ–¹è¯¯å·®", mean_squared_error(y_test, std_y.inverse_transform(y_sgd_predict)))
    # å‡æ–¹è¯¯å·® 0.5325275391867944
```

#### æ‹Ÿåˆæ€§é—®é¢˜

å¯¹äºçº¿æ€§å›å½’æ¥è¯´, è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆä¸»è¦è¿˜æ˜¯é’ˆå¯¹featureçš„æ•°é‡æ¥è§£å†³çš„

è¿‡æ‹Ÿåˆï¼šä¸€ä¸ªå‡è®¾åœ¨è®­ç»ƒæ•°æ®ä¸Šèƒ½å¤Ÿè·å¾—æ¯”å…¶ä»–å‡è®¾æ›´å¥½çš„æ‹Ÿåˆï¼Œ ä½†æ˜¯åœ¨è®­ç»ƒæ•°æ®å¤–çš„æ•°æ®é›†ä¸Šå´ä¸èƒ½å¾ˆå¥½åœ°æ‹Ÿåˆæ•°æ®ï¼Œæ­¤æ—¶è®¤ä¸ºè¿™ä¸ªå‡è®¾å‡ºç°äº†è¿‡æ‹Ÿåˆçš„ç°è±¡ã€‚(æ¨¡å‹è¿‡äºå¤æ‚)

æ¬ æ‹Ÿåˆï¼šä¸€ä¸ªå‡è®¾åœ¨è®­ç»ƒæ•°æ®ä¸Šä¸èƒ½è·å¾—æ›´å¥½çš„æ‹Ÿåˆï¼Œ ä½†æ˜¯åœ¨è®­ç»ƒæ•°æ®å¤–çš„æ•°æ®é›†ä¸Šä¹Ÿä¸èƒ½å¾ˆå¥½åœ°æ‹Ÿåˆæ•°æ®ï¼Œæ­¤æ—¶è®¤ä¸ºè¿™ä¸ªå‡è®¾å‡ºç°äº†æ¬ æ‹Ÿåˆçš„ç°è±¡ã€‚(æ¨¡å‹è¿‡äºç®€å•)

æ‹Ÿåˆçš„ç»“æœä¸€èˆ¬ä½¿ç”¨äº¤å‰éªŒè¯ç»“æœç°è±¡åˆ¤æ–­. å¤§éƒ¨åˆ†æƒ…å†µä¸‹å¦‚æœè®­ç»ƒé›†ç¬¦åˆçš„éƒ½ä¸å¥½é‚£å°±æ˜¯æ¬ æ‹Ÿåˆ, å¦‚æœè®­ç»ƒé›†å®Œç¾æµ‹è¯•é›†è¡¨ç°ä¸å¥½é‚£ä¹ˆå°±æ˜¯è¿‡æ‹Ÿåˆ. ä¸€èˆ¬ä½¿ç”¨å²­å›å½’è§£å†³è¿‡æ‹Ÿåˆ.

ç‰¹å¾é€‰æ‹©:

* è¿‡æ»¤å¼: ä½æ–¹å·®ç‰¹å¾
* åµŒå…¥å¼: æ­£åˆ™åŒ–, å†³ç­–æ ‘, éšæœºæ£®æ—
  * æ­£åˆ™åŒ–: æ›´æ–°æŸä¸ªç‰¹å¾çš„$\theta$å¢å¤§æˆ–è€…å‡å°, æŸ¥çœ‹å¯¹æ¨¡å‹å‡†ç¡®çš„è¦æ±‚. Ridge, å²­å›å½’

#### å²­å›å½’

å²­å›å½’ï¼šå›å½’å¾—åˆ°çš„å›å½’ç³»æ•°æ›´ç¬¦åˆå®é™…ï¼Œæ›´å¯é ã€‚å¦å¤–ï¼Œèƒ½è®©ä¼°è®¡å‚æ•°çš„æ³¢åŠ¨èŒƒå›´å˜å°ï¼Œå˜çš„æ›´ç¨³å®šã€‚åœ¨å­˜åœ¨ç—…æ€æ•°æ®åå¤šçš„ç ”ç©¶ä¸­æœ‰è¾ƒå¤§çš„å®ç”¨ä»·å€¼ã€‚

å²­å›å½’çš„api: `sklearn.linear_model.Ridge(alpha=1.0)`
å…·æœ‰l2æ­£åˆ™åŒ–çš„çº¿æ€§æœ€å°äºŒä¹˜æ³•

* alpha:æ­£åˆ™åŒ–åŠ›åº¦
* coef_:å›å½’ç³»æ•°

```python
# ===========================å²­å›å½’==================================
ridge = Ridge(alpha=1.0)
ridge.fit(x_train, y_train)
print("æ¨¡å‹å‚æ•°: ", ridge.coef_)
y_r_predict = ridge.predict(x_test)
print("æµ‹è¯•é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ:", y_r_predict)
print("å‡æ–¹è¯¯å·®", mean_squared_error(y_test, std_y.inverse_transform(y_r_predict)))

# ç½‘æ ¼æœç´¢ä¸äº¤å‰éªŒè¯
print("æ­£åœ¨è¿›è¡Œç½‘æ ¼å‚æ•°è°ƒä¼˜...");
params = {"alpha": [0.001, 0.005, 0.01, 0.03, 0.07, 0.1, 0.5, 0.7, 1, 10, 50, 100, 500]}
gc = GridSearchCV(ridge, param_grid=params, cv=2)
gc.fit(x_train, y_train)
print("é¢„æµ‹å‡†ç¡®ç‡")
print(gc.score(x_test, y_test))
print("é€‰æ‹©çš„å‚æ•°æ¨¡å‹:")
print(gc.best_params_)
```

#### é€»è¾‘å›å½’

é€»è¾‘å›å½’æ˜¯è§£å†³äºŒåˆ†ç±»é—®é¢˜çš„åˆ©å™¨. ä½¿ç”¨çº¿æ€§å›å½’çš„å…¬å¼å›å½’æ–¹æ³•ä½œä¸ºé€»è¾‘å›å½’çš„å…¬å¼å›å½’æ–¹æ³•.

sigmoidå‡½æ•°

$${S(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{e^{x}+1}}=1-S(-x)}$$

ä½œç”¨å…¬å¼:

$$h_\theta(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}$$

ä¸çº¿æ€§å›å½’åŸç†ç›¸åŒ,ä½†ç”±äºæ˜¯åˆ†ç±»é—®é¢˜ï¼ŒæŸå¤±å‡½æ•°ä¸ä¸€æ ·ï¼Œåªèƒ½é€šè¿‡æ¢¯åº¦ä¸‹é™æ±‚è§£

å¯¹æ•°ä¼¼ç„¶æŸå¤±å‡½æ•°ï¼š

$$cost(h_\theta(x),y) = \left\{\begin{matrix} & - log(h_\theta(x)) & & if & y = 1 \\ & - log(1 - h_\theta(x))) & & if & y = 0 \end{matrix}\right.$$

æŸå¤±å‡½æ•°å›¾åƒ:
![10](./images/10.png)
å½“y=0çš„æ—¶å€™çš„å›¾åƒ:
![11](./images/11.png)

å®Œæ•´çš„æŸå¤±å‡½æ•°:
$$cost(h_\theta(x),y) = -\sum_{i=1}^{m}-y_{i} log(h_\theta(x)) - (1 - y_{i}) log(1 - h_\theta(x))$$

æ¡ˆä¾‹:
å‡è®¾æœ‰4æ ·æœ¬ç»“æœä¸ºtrue,false,false,true, æœ€ç»ˆå‡½æ•°è¿ç®—ç»“æœä¸º 0.6,0.1,0.51,0.7. æœ€ç»ˆç»“æœå°†ä¼šæ˜¯1,0,1,1, ä¹Ÿå°±æ˜¯true,false,true,true. æŸå¤±ä¸º -(1log(0.6) + 0log(0.1) + 0log(0.51) + 1log(0.7))

æ¢¯åº¦ä¸‹é™:

> éœ€è¦æ³¨æ„çš„æ˜¯å‡æ–¹è¯¯å·®åªæœ‰ä¸€ä¸ªæœ€ä½ç‚¹, ä¸å­˜åœ¨å±€éƒ¨æœ€ä½ç‚¹. ä½†æ˜¯å¯¹æ•°æŸå¤±å­˜åœ¨å¤šä¸ªå±€éƒ¨æœ€å°å€¼. æ²¡æœ‰ç»å¯¹çš„è§£å†³æ–¹æ¡ˆ, æœ‰ä¸€äº›å¤„ç†æ–¹æ³•å°½é‡æ”¹å–„, ä½†æ— æ³•å®Œå…¨è§£å†³.
>
> 1. å¤šæ¬¡æ¯”è¾ƒæœ€å°å€¼
> 2. æ±‚è§£çš„è¿‡ç¨‹ä¸­è°ƒæ•´å­¦ä¹ ç‡

é€»è¾‘å›å½’api: `sklearn.linear_model.LogisticRegression(penalty='l2', C = 1.0)` é€»è¾‘å›å½’ä¸­è‡ªå¸¦æ­£åˆ™åŒ–, è§£å†³è¿‡æ‹Ÿåˆçš„é—®é¢˜

* coef_ï¼šå›å½’ç³»æ•°
* penalty: æ­£åˆ™åŒ–è§„åˆ™, l2çš„å½¢å¼
* c: æ­£åˆ™åŒ–çš„åŠ›åº¦

æµ‹è¯•æ•°æ®: <https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/> å¯ä»¥æŸ¥çœ‹breast-cancer-wisconsin.names æ–‡ä»¶æ¥è·å–ç»†èŠ‚

æ•´ç†åä¾‹çš„ç±»å‹å®šä¹‰æ˜¯ä»¥æ¦‚ç‡é…ç½®çš„ æ¦‚ç‡å°çš„å°±æ˜¯æ­£ä¾‹. *æ³¨æ„:ä¸‹è½½æ–‡ä»¶ä¸‹è½½ä¸‹æ¥ä»¥åä¸æ˜¯csvæ ¼å¼çš„,éœ€è¦å®šä¹‰åˆ—å*

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


def logistic_regression():
    """
    é€»è¾‘åˆ†ç±»äºŒåˆ†ç±»è¿›è¡Œç™Œç—‡é¢„æµ‹, æ ¹æ®ç»†èƒç‰¹å¾
    :return:
    """
    # è¯»å–æ•°æ®
    """
    pd.read_csv(â€™â€™,names=column_names)
        column_namesï¼šæŒ‡å®šç±»åˆ«åå­—,['Sample code number','Clump Thickness', 'Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']
        return:æ•°æ®
        
        replace(to_replace=â€™â€™,value=)ï¼šè¿”å›æ•°æ®
        dropna():è¿”å›æ•°æ®
    """
    # æ„é€ åˆ—å
    column_names = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',
                    'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',
                    'Normal Nucleoli', 'Mitoses', 'Class']
    data = pd.read_csv(
        "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data",
        names=column_names)
    print(data)

    # å¯¹ç¼ºå¤±å€¼è¿›è¡Œå¤„ç†, æŠŠ?æ¢æˆnp.nan
    data = data.replace(to_replace='?', value=np.nan)
    data = data.dropna()  # åˆ é™¤æ‰€æœ‰çš„nan, è¿™é‡Œå¯ä»¥é’ˆå¯¹æ¯ä¸€åˆ—è¿›è¡Œå‡å€¼é…ç½®, ä¹Ÿå¯ä»¥æ¯”è¾ƒæ‡’, ç›´æ¥drop

    # æ•°æ®åˆ†éš”
    # train_test_split(data[column_names[1:10]], data['Class'], test_size=0.25)
    # ç‰¹å¾å€¼ä¸ºç¬¬äºŒåˆ—åˆ°ç¬¬10åˆ—, ç»“æœå€¼ä¸ºç¬¬11åˆ—
    x_train, x_test, y_train, y_test = train_test_split(data[column_names[1:10]], data[column_names[10]],
                                                        test_size=0.25)

    # è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†, ç”±äºæ˜¯åˆ†ç±»ç®—æ³•, ä¸éœ€è¦è¿›è¡Œç»“æœæ ‡å‡†åŒ–
    std = StandardScaler()
    x_train = std.fit_transform(x_train)
    x_test = std.fit_transform(x_test)

    # é€»è¾‘å›å½’
    logr = LogisticRegression()  # é»˜è®¤ penalty="l2",C=1.0,
    logr.fit(x_train, y_train)

    print(logr.coef_)
    print("å‡†ç¡®ç‡:", logr.score(x_test, y_test))
    # [[1.49159213 0.46667743 0.92893529 0.75838182 0.12238663 1.18111883 0.75582525 0.30524351 0.85741416]]
    # å‡†ç¡®ç‡: 0.9590643274853801

    # è®¡ç®—ä¸€ä¸‹å¬å›ç‡, è¿™é‡ŒæŒ‡å®šyå€¼çš„ç»“æœ, 2ä»£è¡¨è‰¯æ€§, 4ä»£è¡¨æ¶æ€§
    y_pred = logr.predict(x_test)
    print("å¬å›ç‡:", classification_report(y_test, y_pred, labels=[2, 4], target_names=["è‰¯æ€§", "æ¶æ€§"]))
    # å¬å›ç‡:            precision recall    f1-score   support
    #
    #         è‰¯æ€§       0.95      0.98      0.96       106
    #         æ¶æ€§       0.97      0.91      0.94        65
    #
    #     accuracy                           0.95       171
    #    macro avg       0.96      0.94      0.95       171
    # weighted avg       0.95      0.95      0.95       171
    return None


if __name__ == '__main__':
    logistic_regression()

```

### èšç±»

k-mean éç›‘ç£å­¦ä¹ ç®—æ³•

1. éšæœºè®¾ç½®Kä¸ªç‰¹å¾ç©ºé—´å†…çš„ç‚¹ä½œä¸ºåˆå§‹çš„èšç±»ä¸­å¿ƒ, ä¹Ÿå°±æ˜¯æ‰€æœ‰æ ·æœ¬æœ€ç»ˆåˆ’åˆ†kä¸ªç±»åˆ« (å¦‚æœkä¸æ˜ç¡®,é‚£ä¹ˆå°±æ˜¯è¶…å‚æ•°). éšæœºæŠ½å–ä¸‰ä¸ªæ ·æœ¬, å½“åšä¸‰ä¸ªç±»åˆ«çš„ä¸­å¿ƒç‚¹
2. å¯¹äºå…¶ä»–æ¯ä¸ªç‚¹è®¡ç®—åˆ°Kä¸ªä¸­å¿ƒçš„è·ç¦»ï¼ŒæœªçŸ¥çš„ç‚¹é€‰æ‹©æœ€è¿‘çš„ä¸€ä¸ªèšç±»ä¸­å¿ƒç‚¹ä½œä¸ºæ ‡è®°ç±»åˆ«, è·å–æ¯ä¸ªæ ·æœ¬ç‚¹åˆ°ä¸‰ä¸ªä¸­å¿ƒç‚¹æœ€è¿‘çš„è·ç¦», ç„¶åå¯¹å…¶åˆ†ç±»
3. æ¥ç€å¯¹ç€æ ‡è®°çš„èšç±»ï¼Œé‡æ–°è®¡ç®—å‡ºæ¯ä¸ªèšç±»çš„ä¸­å¿ƒç‚¹ä½ç½®, ç„¶åè·å–å¹³å‡å€¼
4. å¦‚æœè®¡ç®—å¾—å‡ºçš„æ–°ä¸­å¿ƒç‚¹ä¸åŸä¸­å¿ƒç‚¹ä¸€æ ·ï¼Œé‚£ä¹ˆç»“æŸ
5. å¦‚æœä¸åŒ, å°†æ–°çš„ä¸­å¿ƒç‚¹ä½œä¸ºæ–°çš„ä¸­å¿ƒç‚¹, é‡æ–°è¿›è¡Œç¬¬äºŒæ­¥è¿‡ç¨‹

èšç±»api:`sklearn.cluster.KMeans(n_clusters=8,init=â€˜k-means++â€™)`

* n_clusters:å¼€å§‹çš„èšç±»ä¸­å¿ƒæ•°é‡
* init:åˆå§‹åŒ–æ–¹æ³•ï¼Œé»˜è®¤ä¸º'k-means ++â€™
* labels_:é»˜è®¤æ ‡è®°çš„ç±»å‹ï¼Œå¯ä»¥å’ŒçœŸå®å€¼æ¯”è¾ƒï¼ˆä¸æ˜¯å€¼æ¯”è¾ƒï¼‰

è¯„ä¼°æ ‡å‡†: è½®å»“ç³»æ•°
å…¬å¼: $sc_i = \frac{b_i-a_i}{max(b_i,a_i)}$ å¯¹äºæ¯ä¸€ä¸ªæ ·æœ¬éƒ½æœ‰ä¸€ä¸ªç³»æ•°.

å½“ä¸€ä¸ªæ¨¡å‹å»ºç«‹å

1. è®¡ç®—ç¬¬ä¸€ä¸ªæ ·æœ¬åˆ°ç¬¬ä¸€ä¸ªæ ·æœ¬æ‰€åœ¨çš„ç±»åˆ«ä¸­æ‰€æœ‰å…¶ä»–æ ·æœ¬çš„è·ç¦», å–å¹³å‡å€¼ä½œä¸º$a_i$
2. è®¡ç®—ç¬¬ä¸€ä¸ªæ ·æœ¬åˆ°å¦ä¸€ä¸ªç±»åˆ«ä¸­çš„æ‰€æœ‰æ ·æœ¬çš„å¹³å‡è·ç¦»$b_1$, ç„¶ååœ¨è®¡ç®—æ ·æœ¬å¦ä¸€ä¸ª**ç±»åˆ«**çš„å¹³å‡è·ç¦»$b_2$, ä»¥æ­¤ç±»æ¨, æœ‰nä¸ªç±»åˆ«çš„æ—¶å€™, è®¡ç®—å‡ºnä¸ªå¹³å‡è·ç¦»ä»$b_1$åˆ°$b_n$. ç„¶åè·å–æœ€å°çš„bå€¼, æ¯”å¦‚$b_2$, ä½œä¸º$b_i$ç”¨æ¥è¿›è¡Œä¸Šæ–¹çš„å…¬å¼è®¡ç®—.
3. å°†$a_i$å’Œ$b_i$ä»£å…¥å…¬å¼

* $b_i å¤§äº a_i$ çš„æ—¶å€™, ç»“æœå°±ä¼šè¶‹è¿‘äº1, ä¹Ÿæ˜¯ç†æƒ³æ¨¡å‹. åä¹‹ç»“æœä¼šè¶‹è¿‘äº-1, ä¹Ÿæ˜¯æ¯”è¾ƒå·®çš„æ¨¡å‹æˆ–è€…æ— æ³•å®Œç¾åˆ†ç±»

è¯„åˆ†api: `sklearn.metrics.silhouette_score(X,Â labels)` è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡è½®å»“ç³»æ•°

* Xï¼šç‰¹å¾å€¼
* labelsï¼šè¢«èšç±»æ ‡è®°çš„ç›®æ ‡å€¼

```python
import os

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import silhouette_score


def clusters():
    os.environ['OMP_NUM_THREADS'] = "1"
    os.environ['OPENBLAS_NUM_THREADS'] = "1"
    """
    èšç±», åˆ†ä¸º4ä¸ªç±»åˆ«
    :return:
    """
    data = fetch_california_housing()
    x = data.data
    print(x)

    km = KMeans(n_clusters=8)
    km.fit(x)  # è¿›è¡Œåˆ†æ

    predict = km.predict(x)
    print(predict)  # [2 0 2 ... 2 2 0]

    # ç”»ä¸ªå›¾
    plt.figure(figsize=(10, 10))
    # å»ºç«‹4ä¸ªé¢œè‰²çš„åˆ—è¡¨
    colored = ['orange', 'green', 'blue', 'purple', 'cyan', 'magenta', 'yellow', 'red']
    color = [colored[i] for i in predict]  # éå†predict, æ¯ä¸€ä¸ªå®šä¹‰ä¸ºcolored[i], ç»„æˆæ–°çš„arrayæ”¾å…¥color

    plt.scatter(x[:, 1], x[:, 2], color=color)
    # plt.show()

    # èšç±»è¯„ä¼°
    print(silhouette_score(x, predict))  # 0.5242608023536229 å¤§äº0.5 ä¼šæ˜¯æ¯”è¾ƒä¸é”™çš„æ¨¡å‹

    return None


if __name__ == '__main__':
    clusters()
```

### æ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½

åœ¨è®­ç»ƒå®Œä»¥å, å¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œä¿å­˜å’ŒåŠ è½½, å°†æ¥æ¨¡å‹ä½¿ç”¨çš„æ—¶å€™å¯ä»¥ç›´æ¥è°ƒç”¨. å°±æ˜¯ä¸€ä¸ªåºåˆ—åŒ–å’Œè¯»å–çš„å·¥å…·, æ–‡ä»¶åç¼€ä¸ºpkl.

ä¿å­˜åŠ è½½çš„api:`from sklearn.externals import joblib`

* ä¿å­˜: `joblib.dump(rf,'test.pkl')`
* åŠ è½½: `estimator=joblib.load('test.pkl')`

å¯¼å‡ºè®­ç»ƒæ¨¡å‹:

```python
import joblib
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import SGDRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


def linear_regression():
    """
    çº¿æ€§å›å½’ç›´æ¥é¢„æµ‹æˆ¿å±‹ä»·æ ¼
    :return:
    """
    # è·å–æ•°æ®
    data = fetch_california_housing()
    x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25)
    std_x = StandardScaler()
    x_train = std_x.fit_transform(x_train)

    std_y = StandardScaler()
    y_train = std_y.fit_transform(y_train.reshape(-1, 1))

    sgd = SGDRegressor()
    sgd.fit(x_train, y_train)

    # ä¿å­˜è®­ç»ƒæ¨¡å‹
    joblib.dump(sgd, "./export/test.pkl")

    return None


if __name__ == '__main__':
    linear_regression()

```

è¯»å–æ¨¡å‹:

```python
import joblib


def modal_import():
    modal = joblib.load("./export/test.pkl")
    print(modal.coef_)
    return None


if __name__ == '__main__':
    modal_import()
```
