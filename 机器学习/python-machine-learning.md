# æœºå™¨å­¦ä¹ 

åŸºäºpythonçš„machine learningç¬”è®°

ä½¿ç”¨åˆ°çš„åº“å’Œæ¡†æ¶: `Scikit learn`, `Tensorflow`

ç¯å¢ƒæ­å»º: `pip install Scikit-learn`

å¼•ç”¨: `import sklearn`

[toc]

## ç‰¹å¾æŠ½å–

ç‰¹å¾æŠ½å–å°±æ˜¯å°†æ•°æ®è¿›è¡Œç‰¹å¾åŒ–, æ•°å­—åŒ–

sklearnä¸­å­˜åœ¨ç€å¤§é‡çš„ç‰¹å¾æŠ½å–æ–¹æ³•. ä»¥å­—ç¬¦ä¸²ä¸ºä¾‹, æŠ½å–ä¸¤ä¸ªå­—ç¬¦ä¸²çš„æ–‡å­—ç‰¹å¾å±æ€§:

```python
from sklearn.feature_extraction.text import CountVectorizer

# å®ä¾‹åŒ–
vector = CountVectorizer()
# è½¬æ¢æ•°æ®
res = vector.fit_transform(["life is short , i like python", "life is too long, i dislike python"])

print(vector.get_feature_names())
# ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']

print(res.toarray())
# [[0 1 1 1 0 1 1 0]
#  [1 1 1 0 1 1 0 1]]

print(type(res))  # <class 'scipy.sparse._csr.csr_matrix'>
```

ç‰¹å¾æŠ½å–api: `sklearn.feature_extraction`

### å­—å…¸ç‰¹å¾æŠ½å–

- `DictVectorizer` : å­—å…¸æ•°æ®ç‰¹å¾æŠ½å– `DictVectorizer(sparse=True,...)`. å¦‚æœæ•°æ®å·²ç»æ˜¯æœ‰ç±»åˆ«çš„æ•°æ®, éœ€è¦å…ˆè½¬æ¢ä¸ºå­—å…¸ç±»å‹ç„¶åæ‰èƒ½è¯»å–æ•°æ®
  - fit_transform(x): å­—å…¸æˆ–è€…é¥±æ±‰å­ç‚¹çš„è¿­ä»£å™¨è½¬æ¢æˆä¸€ä¸ªsparseçŸ©é˜µ
  - inverse_transform(x): ä¼ å…¥arrayæ•°ç»„æˆ–è€…çŸ©é˜µ, è¿”å›è½¬æ¢ä¹‹å‰çš„æ•°æ®æ ¼å¼
  - get_feature_names(): è¿”å›ç±»åˆ«åç§°
  - transform(x): æŒ‰ç…§åŸå…ˆçš„æ ‡å‡†è½¬æ¢

```python
from sklearn.feature_extraction import DictVectorizer


def dictVec():
    """
    å­—å…¸æŠ½å–æ•°æ®
    :return:
    """
    # å®ä¾‹åŒ–
    dict = DictVectorizer(sparse=False)
    data = dict.fit_transform([{'city': 'åŒ—äº¬', 'temperature': 100},
                               {'city': 'ä¸Šæµ·', 'temperature': 60},
                               {'city': 'æ·±åœ³', 'temperature': 30}])
    print(data)
    # sparse = true
    #   (0, 1) 1.0
    #   (0, 3) 100.0
    #   (1, 0) 1.0
    #   (1, 3) 60.0
    #   (2, 2) 1.0
    #   (2, 3) 30.0
    # sparse = false , ä¹Ÿå°±æ˜¯ndarrayçš„ç±»å‹
    # ä¹Ÿè¢«ç§°ä¸ºone hotç¼–ç 
    # [[  0.   1.   0. 100.]
    #  [  1.   0.   0.  60.]
    #  [  0.   0.   1.  30.]]

    print(dict.inverse_transform(data))
    # è½¬æ¢æˆä¹‹å‰çš„æ•°æ®, ä½†æ˜¯è½¬æ¢æˆç‰¹å¾å€¼çš„ç±»å‹
    # [{'city=åŒ—äº¬': 1.0, 'temperature': 100.0}, {'city=ä¸Šæµ·': 1.0, 'temperature': 60.0}, {'city=æ·±åœ³': 1.0, 'temperature': 30.0}]

    print(dict.get_feature_names_out())  # ['city=ä¸Šæµ·', 'city=åŒ—äº¬', 'city=æ·±åœ³', 'temperature']
    return None


if __name__ == '__main__':
    dictVec()

```

### æ–‡æœ¬ç‰¹å¾æŠ½å–

- æ–‡æœ¬ç‰¹å¾æŠ½å–: `sklearn.feature_extration.text.CountVectorizer`
  - fit_transform(x) æ–‡æœ¬æˆ–è€…åŒ…å«å­—ç¬¦çš„å¯è¿­ä»£å¯¹è±¡è½¬æ¢æˆä¸€ä¸ªçŸ©é˜µè¾“å‡º
  - inverse_transform(x) æ•°ç»„æˆ–è€…sparseçŸ©é˜µè½¬æ¢æˆä¹‹å‰çš„æ•°æ®æ ¼å¼
  - get_feature_names() è·å–å•è¯åˆ—è¡¨
  - é»˜è®¤ä¸æ”¯æŒä¸­æ–‡éœ€è¦å®‰è£…jiebaè¿›è¡Œåˆ†è¯ç„¶åæ‰èƒ½è¿›è¡Œç»Ÿè®¡
    - `pip install jieba`

#### ç¬¬ä¸€ç§æ–¹æ³•

```python
def countVec():
    """
    å¯¹æ–‡æœ¬è¿›è¡Œç‰¹å¾å€¼åŒ–
    ç»Ÿè®¡æ‰€æœ‰æ–‡ç« çš„è¯, é‡å¤çš„åªè®¡ç®—ä¸€æ¬¡, ä½œä¸ºheaders
    é’ˆå¯¹è¿™ä¸ªåˆ—è¡¨, æ¯ä¸€ä¸ªæ–‡ç« ç»Ÿè®¡å•è¯ä¸ªæ•°, æ¯ä¸€ä¸ªæ–‡ç« ç»Ÿè®¡ä¸€æ¬¡, å¯¹äºå•ä¸ªå­—æ¯ä¸ç»Ÿè®¡(å­—æ¯ä¸ä¼šåæ˜ æ–‡ç« ä¸»é¢˜)

    é»˜è®¤ä¸æ”¯æŒä¸­æ–‡æŠ½å–, ä¼˜å…ˆè¿›è¡Œä¸­æ–‡åˆ†è¯
    :return: 
    """
    cv = CountVectorizer()

    data = cv.fit_transform(["life is is short,i like python", "life is too long,i dislike python"])
    print(data.toarray())
    # [[0 2 1 1 0 1 1 0]
    #  [1 1 1 0 1 1 0 1]]

    print(cv.get_feature_names_out())
    # ['dislike' 'is' 'life' 'like' 'long' 'python' 'short' 'too']

    return None


def cutword():
    con1 = jieba.cut("1ã€ä»Šå¤©å¾ˆæ®‹é…·ï¼Œæ˜å¤©æ›´æ®‹é…·ï¼Œåå¤©å¾ˆç¾å¥½ï¼Œä½†ç»å¯¹å¤§éƒ¨åˆ†æ˜¯æ­»åœ¨æ˜å¤©æ™šä¸Šï¼Œæ‰€ä»¥æ¯ä¸ªäººä¸è¦æ”¾å¼ƒä»Šå¤©ã€‚")
    con2 = jieba.cut("2ã€æˆ‘ä»¬çœ‹åˆ°çš„ä»å¾ˆè¿œæ˜Ÿç³»æ¥çš„å…‰æ˜¯åœ¨å‡ ç™¾ä¸‡å¹´ä¹‹å‰å‘å‡ºçš„ï¼Œè¿™æ ·å½“æˆ‘ä»¬çœ‹åˆ°å®‡å®™æ—¶ï¼Œæˆ‘ä»¬æ˜¯åœ¨çœ‹å®ƒçš„è¿‡å»ã€‚")
    con3 = jieba.cut("3ã€å¦‚æœåªç”¨ä¸€ç§æ–¹å¼äº†è§£æŸæ ·äº‹ç‰©ï¼Œä½ å°±ä¸ä¼šçœŸæ­£äº†è§£å®ƒã€‚äº†è§£äº‹ç‰©çœŸæ­£å«ä¹‰çš„ç§˜å¯†å–å†³äºå¦‚ä½•å°†å…¶ä¸æˆ‘ä»¬æ‰€äº†è§£çš„äº‹ç‰©ç›¸è”ç³»ã€‚")

    # è½¬æ¢æˆåˆ—è¡¨
    content1 = list(con1)
    print(content1)
    content2 = list(con2)
    print(content2)
    content3 = list(con3)
    print(content3)

    # è½¬æ¢æˆå­—ç¬¦ä¸²
    c1 = " ".join(content1)
    c2 = " ".join(content2)
    c3 = " ".join(content3)
    return c1, c2, c3


def chinese_vec():
    """
    ä¸­æ–‡æ–‡æœ¬æŠ½å–
    :return:
    """
    cv = CountVectorizer()
    c1, c2, c4 = cutword()
    data = cv.fit_transform([c1, c2, c4])
    print(cv.get_feature_names_out())
    # ['ä¸€ç§', 'ä¸ä¼š', 'ä¸è¦', 'ä¹‹å‰', 'äº†è§£', 'äº‹ç‰©', 'ä»Šå¤©', 'å…‰æ˜¯åœ¨', 'å‡ ç™¾ä¸‡å¹´', 'å‘å‡º', 'å–å†³äº', 'åªç”¨', 'åå¤©', 'å«ä¹‰', 'å¤§éƒ¨åˆ†', 'å¦‚ä½•', 'å¦‚æœ', 'å®‡å®™',
    #  'æˆ‘ä»¬', 'æ‰€ä»¥', 'æ”¾å¼ƒ', 'æ–¹å¼', 'æ˜å¤©', 'æ˜Ÿç³»', 'æ™šä¸Š', 'æŸæ ·', 'æ®‹é…·', 'æ¯ä¸ª', 'çœ‹åˆ°', 'çœŸæ­£', 'ç§˜å¯†', 'ç»å¯¹', 'ç¾å¥½', 'è”ç³»', 'è¿‡å»', 'è¿™æ ·']

    print(data.toarray())
    # [[0 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 0]
    #  [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 1]
    #  [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0]]
```

#### ç¬¬äºŒç§æ–¹æ³•

tfidfæ–‡æœ¬åˆ†ç±». æ–‡æœ¬ç‰¹å¾æŠ½å–åˆ†ç±».

- TF Term frequency, è¯é¢‘

$$tf_{i,j} = \frac{n_{i,j}}{\sum_kn_{k,j}}$$

- IDF inverse document frequency, é€†æ–‡æ¡£é¢‘ç‡
$$idf_i = lg\frac{|D|}{|\{j:t_i \in d_j\}|}$$
$|D|$ï¼šè¯­æ–™åº“ä¸­çš„æ–‡ä»¶æ€»æ•°
$|\{j:t_{i}\in d_{j}\}|$ï¼šåŒ…å«è¯è¯­$t_{{i}}$çš„æ–‡ä»¶æ•°ç›®ï¼ˆå³$n_{{i,j}}\neq 0$çš„æ–‡ä»¶æ•°ç›®ï¼‰å¦‚æœè¯è¯­ä¸åœ¨èµ„æ–™ä¸­ï¼Œå°±å¯¼è‡´åˆ†æ¯ä¸ºé›¶ï¼Œå› æ­¤ä¸€èˆ¬æƒ…å†µä¸‹ä½¿ç”¨${1+|\{j:t_{i}\in d_{j}\}|}$.

ä¹Ÿå°±æ˜¯log(æ€»æ–‡æ¡£æ•°/è¯¥è¯å‡ºç°çš„æ–‡æ¡£æ•°é‡)

sklearnçš„ç±»: `sklearn.feature_extraction.text.TfidfVectorizer`

```python
def tfidf_vec():
    tfidf_vector = TfidfVectorizer()
    c1, c2, c3 = cutword()
    data = tfidf_vector.fit_transform(["life is is short,i like python", "life is too long,i dislike python"])
    print(tfidf_vector.get_feature_names_out(data))
    print(data.toarray())
    # è¾“å‡ºç»“æœ -> æ•°å€¼è¡¨ç¤ºçš„æ˜¯é‡è¦æ€§
    # ['dislike' 'is' 'life' 'like' 'long' 'python' 'short' 'too']
    # [[0.         0.63402146 0.31701073 0.44554752 0.         0.31701073
    #   0.44554752 0.        ]
    #  [0.47042643 0.33471228 0.33471228 0.         0.47042643 0.33471228
    #   0.         0.47042643]]
```

### ç‰¹å¾é¢„å¤„ç† (æ•°æ®)

å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†: é€šè¿‡ç‰¹å®šçš„æ•°å­¦ç»Ÿè®¡æ–¹æ³•, å°†æ•°æ®è½¬æ¢ä¸ºç®—æ³•è¦æ±‚çš„æ•°æ®. ç›®çš„ä½¿å¾—ä¸€ä¸ªç‰¹å¾å¯¹æœ€ç»ˆç»“æœä¸ä¼šé€ æˆè¿‡å¤§çš„å½±å“.

![é¢„å¤„ç†](./images/Snipaste_2022-03-05_20-37-17.png)

é’ˆå¯¹äºsklearnçš„é¢„å¤„ç†æ–¹æ³•éƒ½å­˜å‚¨åœ¨`sklearn. preprocessing`ä¸­

#### å½’ä¸€åŒ– Normalization

å¦‚æœéœ€è¦å¤šä¸ªç‰¹å¾åŒç­‰é‡è¦çš„æ—¶å€™å°±å¯ä»¥ç”¨å½’ä¸€åŒ–. ä½†æ˜¯å½’ä¸€åŒ–å¯¹å¼‚å¸¸ç‚¹çš„å¤„ç†æ¯”è¾ƒå›°éš¾.

å…¬å¼:

$$X' = \frac{x - min}{max - min}$$ $$X'' = X' * (mx-mi) + mi$$

ä½œç”¨äºæ¯ä¸€åˆ—ï¼Œmaxä¸ºä¸€åˆ—çš„æœ€å¤§å€¼ï¼Œminä¸ºä¸€åˆ—çš„æœ€å°å€¼,é‚£ä¹ˆXâ€™â€™
ä¸ºæœ€ç»ˆç»“æœï¼Œmxï¼Œmiåˆ†åˆ«ä¸ºæŒ‡å®šåŒºé—´å€¼é»˜è®¤mxä¸º1,miä¸º0

ç›¸å¯¹äºä¸Šå›¾ä¸­ç¬¬ä¸€ç»„æ•°æ®çš„ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—å’Œç¬¬ä¸€è¡Œç¬¬äºŒåˆ—çš„æ•°æ®ä¸º
![è®¡ç®—](./images/2.png)

**sklearnå½’ä¸€åŒ–API:  `sklearn.preprocessing.MinMaxScaler`**

```python
from sklearn.preprocessing import MinMaxScaler
def normalization():
    """
    å½’ä¸€åŒ–
    :return:
    """
    normalizer = MinMaxScaler()
    data = normalizer.fit_transform([[90, 2, 10, 40],
                                     [60, 4, 15, 45],
                                     [75, 3, 13, 46]])
    print(data)
    # [[1.         0.         0.         0.        ]
    #  [0.         1.         1.         0.83333333]
    #  [0.5        0.5        0.6        1.        ]]

    normalizer = MinMaxScaler(feature_range=(2, 3))
    data = normalizer.fit_transform([[90, 2, 10, 40],
                                     [60, 4, 15, 45],
                                     [75, 3, 13, 46]])
    print(data)
    # [[3.         2.         2.         2.        ]
    #  [2.         3.         3.         2.83333333]
    #  [2.5        2.5        2.6        3.        ]]
    return None
```

*æ³¨æ„åœ¨ç‰¹å®šåœºæ™¯ä¸‹æœ€å¤§å€¼æœ€å°å€¼æ˜¯å˜åŒ–çš„ï¼Œå¦å¤–ï¼Œæœ€å¤§å€¼ä¸æœ€å°å€¼éå¸¸å®¹æ˜“å—å¼‚å¸¸ç‚¹å½±å“ï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•é²æ£’æ€§è¾ƒå·®ï¼Œåªé€‚åˆä¼ ç»Ÿç²¾ç¡®å°æ•°æ®åœºæ™¯ã€‚*

---

#### æ ‡å‡†åŒ–

é€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®å˜æ¢åˆ°å‡å€¼ä¸º0,æ–¹å·®ä¸º1èŒƒå›´å†…

**å…¬å¼:**

$$X' = \frac{x - mean}{\sigma}$$

ä½œç”¨äºæ¯ä¸€åˆ—ï¼Œmeanä¸ºå¹³å‡å€¼ï¼Œğœä¸ºæ ‡å‡†å·®(è€ƒé‡æ•°æ®çš„ç¨³å®šæ€§)

stdæˆä¸ºæ–¹å·®:

$$ğ‘ ğ‘¡ğ‘‘=\frac{((ğ‘¥1âˆ’ğ‘šğ‘’ğ‘ğ‘›)^2+(ğ‘¥2âˆ’ğ‘šğ‘’ğ‘ğ‘›)^2+â€¦)}{(ğ‘›(æ¯ä¸ªç‰¹å¾çš„æ ·æœ¬æ•°))}ï¼Œ\sigma = \sqrt{std}$$

sklearnç‰¹å¾åŒ–API: `scikit-learn.preprocessing.StandardScaler`

```python
from sklearn.preprocessing import StandardScaler
def standarlization():
    """
    æ ‡å‡†åŒ–ç¼©æ”¾
    :return:
    """
    std = StandardScaler()
    data = std.fit_transform([[1., -1., 3.],
                              [2., 4., 2.],
                              [4., 6., -1.]])
    print(data)
    # [[-1.06904497 -1.35873244  0.98058068]
    #  [-0.26726124  0.33968311  0.39223227]
    #  [ 1.33630621  1.01904933 -1.37281295]]
    return None
```

#### ç¼ºå¤±å€¼çš„å¤„ç†æ–¹æ³•

å¦‚æœæ¯åˆ—æˆ–è€…è¡Œæ•°æ®ç¼ºå¤±å€¼è¾¾åˆ°ä¸€å®šçš„æ¯”ä¾‹ï¼Œå»ºè®®æ”¾å¼ƒæ•´è¡Œæˆ–è€…æ•´åˆ—

å¯ä»¥é€šè¿‡ç¼ºå¤±å€¼æ¯è¡Œæˆ–è€…æ¯åˆ—çš„å¹³å‡å€¼ã€ä¸­ä½æ•°æ¥å¡«å……

sklearnç¼ºå¤±å€¼API: `sklearn.preprocessing.Imputer`

*æ³¨æ„: æ–°ç‰ˆæœ¬ä¸­ä¸å­˜åœ¨è¿™ä¸ªç±», åœ¨ä½ç‰ˆæœ¬ä¸­å­˜åœ¨, åŒ…ç‰ˆæœ¬ä¸­å¯ä»¥ä½¿ç”¨SimpleImputer. åŒæ—¶sklearn.imputeä¸­å­˜åœ¨å…¶ä»–çš„imputerç±»å¯ä»¥ä½¿ç”¨*

```python
from sklearn.impute import SimpleImputer
def imputer():
    """
    ç¼ºå¤±å€¼å¤„ç†
    :return:
    """
    # æ›¿æ¢ç­–ç•¥:
    # "mean"ï¼Œä½¿ç”¨è¯¥åˆ—çš„å¹³å‡å€¼æ›¿æ¢ç¼ºå¤±å€¼ã€‚ä»…ç”¨äºæ•°å€¼æ•°æ®ï¼›
    # "median"ï¼Œä½¿ç”¨è¯¥åˆ—çš„ä¸­ä½æ•°æ›¿æ¢ç¼ºå¤±å€¼ã€‚ä»…ç”¨äºæ•°å€¼æ•°æ®ï¼›
    # "most_frequent"ï¼Œä½¿ç”¨æ¯ä¸ªåˆ—ä¸­æœ€å¸¸è§çš„å€¼æ›¿æ¢ç¼ºå¤±å€¼ã€‚å¯ç”¨äºéæ•°å€¼æ•°æ®ï¼›
    # "constant"ï¼Œç”¨fill_valueæ›¿æ¢ç¼ºå¤±å€¼ã€‚å¯ç”¨äºéæ•°å€¼æ•°æ®
    imp = SimpleImputer(missing_values=np.nan, strategy='mean')
    data = imp.fit_transform([[1, 2],
                              [np.nan, 3],
                              [7, 6]])
    print(data)
    # [[1. 2.]
    #  [4. 3.]
    #  [7. 6.]]
    return None
```

#### æ•°æ®é™ç»´

æ•°æ®é™ç»´çš„æ„æ€æ˜¯å‡å°‘æ•°æ®ç‰¹å¾çš„æ•°é‡

ç‰¹å¾é™ç»´çš„åŸå› :

- å†—ä½™ï¼šéƒ¨åˆ†ç‰¹å¾çš„ç›¸å…³åº¦é«˜ï¼Œå®¹æ˜“æ¶ˆè€—è®¡ç®—æ€§èƒ½

- å™ªå£°ï¼šéƒ¨åˆ†ç‰¹å¾å¯¹é¢„æµ‹ç»“æœæœ‰è´Ÿå½±å“

**æ–¹å·®è¿‡æ»¤**

ç‰¹å¾è¿‡æ»¤çš„æ–¹æ³• `sklearn.feature_selection.VarianceThreshold`

```python
def variance():
    """
    åˆ é™¤ä½æ–¹å·®çš„ç‰¹å¾
    """
    var = VarianceThreshold(threshold=0.00001)  # åˆ é™¤æ–¹å·®ä½äº0.00001çš„æ•°æ®
    data = var.fit_transform([[0, 2, 0, 3],
                              [0, 1, 4, 3],
                              [0, 1, 1, 3]])
    print(data)
    # [[2 0]
    #  [1 4]
    #  [1 1]]
    return None
```

**ä¸»æˆåˆ†åˆ†æ (PCA)**

æœ¬è´¨ï¼šPCAæ˜¯ä¸€ç§åˆ†æã€ç®€åŒ–æ•°æ®é›†çš„æŠ€æœ¯

ç›®çš„ï¼šæ˜¯æ•°æ®ç»´æ•°å‹ç¼©ï¼Œå°½å¯èƒ½é™ä½åŸæ•°æ®çš„ç»´æ•°ï¼ˆå¤æ‚åº¦ï¼‰ï¼ŒæŸå¤±å°‘é‡ä¿¡æ¯ã€‚

ä½œç”¨ï¼šå¯ä»¥å‰Šå‡å›å½’åˆ†ææˆ–è€…èšç±»åˆ†æä¸­ç‰¹å¾çš„æ•°é‡

PCAçš„æ•°å­¦å®šä¹‰æ˜¯ï¼šä¸€ä¸ªæ­£äº¤åŒ–çº¿æ€§å˜æ¢ï¼ŒæŠŠæ•°æ®å˜æ¢åˆ°ä¸€ä¸ªæ–°çš„åæ ‡ç³»ç»Ÿä¸­ï¼Œä½¿å¾—è¿™ä¸€æ•°æ®çš„ä»»ä½•æŠ•å½±çš„ç¬¬ä¸€å¤§æ–¹å·®åœ¨ç¬¬ä¸€ä¸ªåæ ‡ï¼ˆç§°ä¸ºç¬¬ä¸€ä¸»æˆåˆ†ï¼‰ä¸Šï¼Œç¬¬äºŒå¤§æ–¹å·®åœ¨ç¬¬äºŒä¸ªåæ ‡ï¼ˆç¬¬äºŒä¸»æˆåˆ†ï¼‰ä¸Šï¼Œä¾æ¬¡ç±»æ¨

n_components: ä¸€èˆ¬ç”¨å°æ•°, è¯´æ˜ä¿ç•™çš„ç‰¹å¾æ•°æ®. æŠŠéƒ¨åˆ†æ•°æ®å‡å°‘åˆ°90%. å¦‚æœä½¿ç”¨æ•´æ•°çš„è¯, å°±è¡¨æ˜éœ€è¦é™åˆ°å¤šå°‘ä¸ªç‰¹å¾, ä¸€èˆ¬ä¸ç”¨

```python
def pca():
    """
    ä¸»æˆåˆ†åˆ†æè¿›è¡Œæ•°æ®é™ç»´
    :return:
    """
    p = PCA(n_components=0.9)
    data = p.fit_transform([[2, 8, 4, 5],
                            [6, 3, 0, 8],
                            [5, 4, 9, 1]])
    print(data)
    # [[ 1.28620952e-15  3.82970843e+00]
    #  [ 5.74456265e+00 -1.91485422e+00]
    #  [-5.74456265e+00 -1.91485422e+00]]
    return None
```

## æ¨¡å‹

é’ˆå¯¹ä¸åŒçš„æ•°æ®éœ€è¦ä½¿ç”¨ä¸åŒçš„æ•°æ®ç±»å‹. *åªè¦è®°ä½ä¸€ç‚¹ï¼Œç¦»æ•£å‹æ˜¯åŒºé—´å†…ä¸å¯åˆ†ï¼Œè¿ç»­å‹æ˜¯åŒºé—´å†…å¯åˆ†*

- ç¦»æ•£å‹æ•°æ®ï¼šç”±è®°å½•ä¸åŒç±»åˆ«ä¸ªä½“çš„æ•°ç›®æ‰€å¾—åˆ°çš„æ•°æ®ï¼Œåˆç§°è®¡æ•°æ•°æ®ï¼Œæ‰€æœ‰è¿™äº›æ•°æ®å…¨éƒ¨éƒ½æ˜¯æ•´æ•°ï¼Œè€Œä¸”ä¸èƒ½å†ç»†åˆ†ï¼Œä¹Ÿä¸èƒ½è¿›ä¸€æ­¥æé«˜ä»–ä»¬çš„ç²¾ç¡®åº¦ã€‚
- è¿ç»­å‹æ•°æ®ï¼šå˜é‡å¯ä»¥åœ¨æŸä¸ªèŒƒå›´å†…å–ä»»ä¸€æ•°ï¼Œå³å˜é‡çš„å–å€¼å¯ä»¥æ˜¯è¿ç»­çš„ï¼Œå¦‚ï¼Œé•¿åº¦ã€æ—¶é—´ã€è´¨é‡å€¼ç­‰ï¼Œè¿™ç±»æ•´æ•°é€šå¸¸æ˜¯éæ•´æ•°ï¼Œå«æœ‰å°æ•°éƒ¨åˆ†ã€‚

æœºå™¨å­¦ä¹ çš„åˆ†ç±»

- ç›‘ç£å­¦ä¹  (ç‰¹å¾å€¼åŠ ç›®æ ‡å€¼)
  - åˆ†ç±»(ç›®æ ‡å€¼ç¦»æ•£å‹æ•°æ®) k-è¿‘é‚»ç®—æ³•ã€è´å¶æ–¯åˆ†ç±»ã€å†³ç­–æ ‘ä¸éšæœºæ£®æ—ã€é€»è¾‘å›å½’ã€ç¥ç»ç½‘ç»œ
  - å›å½’(ç›®æ ‡å€¼è¿ç»­å‹æ•°æ®) çº¿æ€§å›å½’ã€å²­å›å½’
  - æ ‡æ³¨ éšé©¬å°”å¯å¤«æ¨¡å‹
- æ— ç›‘ç£å­¦ä¹  (åªæœ‰ç‰¹å¾å€¼)
  - èšç±» k-means

1. æ˜ç¡®æ•°æ®éœ€è¦åšä»€ä¹ˆ
2. å¤„ç†æ•°æ®
3. ç‰¹å¾å·¥ç¨‹, å¤„ç†ç‰¹å¾
4. æ‰¾åˆ°åˆé€‚çš„ç®—æ³•è¿›è¡Œé¢„æµ‹
5. è¯„ä¼°è®¡ç®—ç»“æœæ¨¡å‹ (å¦‚æœæˆåŠŸ,é‚£ä¹ˆéƒ¨ç½²,å¦‚æœæ²¡æˆåŠŸ,å¯ä»¥è°ƒæ•´å‚æ•°æˆ–è€…æ¢ç®—æ³•,é‡æ–°å°è¯•ç‰¹å¾å·¥ç¨‹)
6. ä½¿ç”¨, ä»¥apiå½¢å¼æä¾›

ç›‘ç£å­¦ä¹ ï¼ˆè‹±è¯­ï¼šSupervised learningï¼‰ï¼Œå¯ä»¥ç”±è¾“å…¥æ•°æ®ä¸­å­¦åˆ°æˆ–å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä¾æ­¤æ¨¡å¼æ¨æµ‹æ–°çš„ç»“æœã€‚è¾“å…¥æ•°æ®æ˜¯ç”±è¾“å…¥ç‰¹å¾å€¼å’Œç›®æ ‡å€¼æ‰€ç»„æˆã€‚å‡½æ•°çš„è¾“å‡ºå¯ä»¥æ˜¯ä¸€ä¸ªè¿ç»­çš„å€¼ï¼ˆç§°ä¸ºå›å½’ï¼‰ï¼Œæˆ–æ˜¯è¾“å‡ºæ˜¯æœ‰é™ä¸ªç¦»æ•£å€¼ï¼ˆç§°ä½œåˆ†ç±»ï¼‰ã€‚

æ— ç›‘ç£å­¦ä¹ ï¼ˆè‹±è¯­ï¼šSupervised learningï¼‰ï¼Œå¯ä»¥ç”±è¾“å…¥æ•°æ®ä¸­å­¦åˆ°æˆ–å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä¾æ­¤æ¨¡å¼æ¨æµ‹æ–°çš„ç»“æœã€‚è¾“å…¥æ•°æ®æ˜¯ç”±è¾“å…¥ç‰¹å¾å€¼æ‰€ç»„æˆã€‚

### æ•°æ®åˆ’åˆ†

é€šå¸¸æƒ…å†µä¸‹è®­ç»ƒé›†çš„æ•°æ®å’Œæµ‹è¯•é›†çš„æ•°æ®åˆ’åˆ†ä¸º7:3æˆ–è€…3:1. å¯ä»¥ä½¿ç”¨pythonæ ·æœ¬åˆ’åˆ†å·¥å…·.

æ•°æ®é›†åˆ’åˆ†api `sklearn.model_selection.train_test_split`

load*å’Œfetch*è¿”å›çš„æ•°æ®ç±»å‹datasets.base.Bunch(å­—å…¸æ ¼å¼)

- dataï¼šç‰¹å¾æ•°æ®æ•°ç»„ï¼Œæ˜¯ [n_samples * n_features] çš„äºŒç»´ numpy.ndarray æ•°ç»„
- targetï¼šæ ‡ç­¾æ•°ç»„ï¼Œæ˜¯ n_samples çš„ä¸€ç»´ numpy.ndarray æ•°ç»„
- DESCRï¼šæ•°æ®æè¿°
- feature_namesï¼šç‰¹å¾å,æ–°é—»æ•°æ®ï¼Œæ‰‹å†™æ•°å­—ã€å›å½’æ•°æ®é›†æ²¡æœ‰
- target_namesï¼šæ ‡ç­¾å,å›å½’æ•°æ®é›†æ²¡æœ‰

åˆ†ç±»æ•°æ®é›†çš„æ ¼å¼

sklearn.datasets.load_iris() åŠ è½½å¹¶è¿”å›é¸¢å°¾èŠ±æ•°æ®é›†, ä¸€ç»„æµ‹è¯•æ•°æ®. åˆ†ç±»ç±»å‹æ•°æ®

```python
from sklearn.datasets import load_iris

li = load_iris()
print("è·å–ç‰¹å¾å€¼")
print(li.data)
# è·å–ç‰¹å¾å€¼
# [[5.1 3.5 1.4 0.2]
#  [4.9 3.  1.4 0.2]
#  [4.7 3.2 1.3 0.2]
# ...

print("ç›®æ ‡å€¼")
print(li.target)
# ç›®æ ‡å€¼
# [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
#  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
#  2 2]

print(li.DESCR)
# å±•ç¤ºèŠ±çš„ç‰¹å¾
```

sklearn.datasets.load_digits() åŠ è½½å¹¶è¿”å›æ•°å­—æ•°æ®é›†

#### æ•°æ®é›†åˆ†å‰²

API: `sklearn.model_selection.train_test_split(*arrays,Â **options)`

- x æ•°æ®é›†çš„ç‰¹å¾å€¼
- y æ•°æ®é›†çš„æ ‡ç­¾å€¼
- test_size æµ‹è¯•é›†çš„å¤§å°ï¼Œä¸€èˆ¬ä¸ºfloat
- random_state éšæœºæ•°ç§å­,ä¸åŒçš„ç§å­ä¼šé€ æˆä¸åŒçš„éšæœºé‡‡æ ·ç»“æœã€‚ç›¸åŒçš„ç§å­é‡‡æ ·ç»“æœç›¸åŒã€‚
- return  è®­ç»ƒé›†ç‰¹å¾å€¼ï¼Œæµ‹è¯•é›†ç‰¹å¾å€¼ï¼Œè®­ç»ƒæ ‡ç­¾ï¼Œæµ‹è¯•æ ‡ç­¾ (é»˜è®¤éšæœºå–)

```python
# è¿”å›å€¼:è®­ç»ƒé›†x_train,y_train. æµ‹è¯•é›†x_test,y_test
x_train, x_test, y_train, y_test = train_test_split(li.data, li.target, test_size=0.25)
print("è®­ç»ƒé›†ç‰¹å¾å€¼å’Œç›®æ ‡å€¼", x_train, y_train)
print("æµ‹è¯•é›†ç‰¹å¾å€¼å’Œç›®æ ‡å€¼", x_test, y_test)
```

ä¸‹è½½ä¸€ä¸ªç”¨äºåˆ†ç±»çš„å¤§æ•°æ®é›†, è¿™é‡Œä¼šä¸‹è½½ä¸€ä¸ªæµ‹è¯•æ•°æ®é›†åœ¨pythonçš„homeç›®å½•ä¸­(*æ³¨æ„:æ•°æ®é‡è¶…å¤§, æ²¡äº‹åˆ«æµª*):

`sklearn.datasets.fetch_20newsgroups(data_home=None,subset=â€˜trainâ€™)`

subset: 'train'æˆ–è€…'test','all'ï¼Œå¯é€‰ï¼Œé€‰æ‹©è¦åŠ è½½çš„æ•°æ®é›†.è®­ç»ƒé›†çš„â€œè®­ç»ƒâ€ï¼Œæµ‹è¯•é›†çš„â€œæµ‹è¯•â€ï¼Œä¸¤è€…çš„â€œå…¨éƒ¨â€

å¯ä»¥ä½¿ç”¨å‘½ä»¤`datasets.clear_data_home(data_home=None)`æ¥æ¸…é™¤ç›®å½•ä¸‹çš„æ•°æ®

```python
news = fetch_20newsgroups(subset='all')
print(news.data)
print(news.target)
```

ä¸‹è½½ä¸€ä¸ªç”¨äºå›å½’çš„å¤§æ•°æ®é›†

`sklearn.datasets.load_boston()` åŠ è½½å¹¶è¿”å›æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†
`sklearn.datasets.load_diabetes()` åŠ è½½å’Œè¿”å›ç³–å°¿ç—…æ•°æ®é›†

```python
lb = load_boston()
print(lb.data)  # ç‰¹å¾å€¼
print(lb.target)  # ç›®æ ‡å€¼
print(lb.DESCR)
```

#### è½¬æ¢å™¨, é¢„ä¼°å™¨

**è½¬æ¢å™¨**

```python
from sklearn.preprocessing import StandardScaler

s = StandardScaler()
s.fit_transform([[1, 2, 3], [4, 5, 6]])

ss = StandardScaler();
ss.fit([[1, 2, 3], [4, 5, 6]])
print(ss.transform([[1, 2, 3], [4, 5, 6]]))
# [[-1. -1. -1.]
#  [ 1.  1.  1.]]

# fit_transform = fit + transform

ss.fit([[1, 2, 3], [4, 5, 7]])  # æ­¤å¤„è¿ç®—çš„æ ‡å‡†å·®å’Œæ–¹å·®
print(ss.transform([[1, 2, 3], [4, 5, 6]]))  # ç”±äºæ ‡å‡†å·®fitè®¡ç®—å‡ºæ¥çš„ä¸ä¸€æ ·,å› æ­¤ç»“æœä¸åŒ
# [[-1.  -1.  -1. ]
#  [ 1.   1.   0.5]]

# ä¹Ÿå¯ä»¥é€šè¿‡æ•°æ®åˆ‡åˆ†åˆ’åˆ†æ•°æ®
# ç¼©å°æ•°æ®, é€šè¿‡queryæŸ¥è¯¢æ•°æ®
data = data.query("x > 1.0 & x < 1.25 & y > 2.5 & < 2.75")
```

**ä¼°è®¡å™¨**

åœ¨sklearnä¸­ï¼Œä¼°è®¡å™¨(estimator)æ˜¯ä¸€ä¸ªé‡è¦çš„è§’è‰²ï¼Œåˆ†ç±»å™¨å’Œå›å½’å™¨éƒ½å±äºestimatorï¼Œæ˜¯ä¸€ç±»å®ç°äº†ç®—æ³•çš„API

1ã€ç”¨äºåˆ†ç±»çš„ä¼°è®¡å™¨ï¼š

- sklearn.neighbors k-è¿‘é‚»ç®—æ³•
- sklearn.naive_bayes è´å¶æ–¯
- sklearn.linear_model.LogisticRegression é€»è¾‘å›å½’
- sklearn.tree å†³ç­–æ ‘ä¸éšæœºæ£®æ—

2ã€ç”¨äºå›å½’çš„ä¼°è®¡å™¨ï¼š

- sklearn.linear_model.LinearRegression çº¿æ€§å›å½’
- sklearn.linear_model.Ridge å²­å›å½’

#### è¯„ä¼°æ ‡å‡†:ç²¾ç¡®ç‡(Precision)ä¸å¬å›ç‡(Recall)

ç²¾ç¡®ç‡ï¼šé¢„æµ‹ç»“æœä¸ºæ­£ä¾‹æ ·æœ¬ä¸­çœŸå®ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼ˆæŸ¥å¾—å‡†ï¼‰
![image](./images/3.png)

å¬å›ç‡ï¼šçœŸå®ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­é¢„æµ‹ç»“æœä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼ˆæŸ¥çš„å…¨ï¼Œå¯¹æ­£æ ·æœ¬çš„åŒºåˆ†èƒ½åŠ›ï¼‰
![image](./images/4.png)

$$F1 = \frac{2TP}{2TP + FN + FP} = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}$$

åˆ†ç±»è¯„ä¼°çš„api`sklearn.metrics.classification_report`

`sklearn.metrics.classification_report(y_true,Â y_pred,Â target_names=None)`

- y_trueï¼šçœŸå®ç›®æ ‡å€¼
- y_predï¼šä¼°è®¡å™¨é¢„æµ‹ç›®æ ‡å€¼
- target_namesï¼šç›®æ ‡ç±»åˆ«åç§°
- returnï¼šæ¯ä¸ªç±»åˆ«ç²¾ç¡®ç‡ä¸å¬å›ç‡

```python
print("æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡\n", classification_report(y_test, y_predict, target_names=news.target_names))
# æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
#                            precision    recall  f1-score   support
#
#              alt.atheism       0.89      0.77      0.83       201
#            comp.graphics       0.93      0.78      0.85       256
#  comp.os.ms-windows.misc       0.86      0.81      0.84       261
# comp.sys.ibm.pc.hardware       0.74      0.85      0.79       255
#    comp.sys.mac.hardware       0.88      0.86      0.87       231
# ...
```

##### äº¤å‰éªŒè¯, ç½‘æ ¼æœç´¢(è¶…å‚æ•°æœç´¢)

äº¤å‰éªŒè¯ï¼šå°†æ‹¿åˆ°çš„æ•°æ®ï¼Œåˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯é›†ã€‚ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼šå°†æ•°æ®åˆ†æˆ5ä»½ï¼Œå…¶ä¸­ä¸€ä»½ä½œä¸ºéªŒè¯é›†ã€‚ç„¶åç»è¿‡5æ¬¡(ç»„)çš„æµ‹è¯•ï¼Œæ¯æ¬¡éƒ½æ›´æ¢ä¸åŒçš„éªŒè¯é›†ã€‚å³å¾—åˆ°5ç»„æ¨¡å‹çš„ç»“æœï¼Œ**å–å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆç»“æœ**ã€‚ç”±äºåˆ†ä¸ºäº†5ç»„æ‰€ä»¥ç§°ä¸º5æŠ˜äº¤å‰éªŒè¯, ä¹Ÿå¯ä»¥ä½¿ç”¨4æŠ˜äº¤å‰éªŒè¯ã€‚
![image](./images/5.png)

ç½‘æ ¼æœç´¢: é€šå¸¸æƒ…å†µä¸‹ï¼Œæœ‰å¾ˆå¤šå‚æ•°æ˜¯éœ€è¦æ‰‹åŠ¨æŒ‡å®šçš„ï¼ˆå¦‚k-è¿‘é‚»ç®—æ³•ä¸­çš„Kå€¼ï¼‰ï¼Œè¿™ç§å«è¶…å‚æ•°ã€‚ä½†æ˜¯æ‰‹åŠ¨è¿‡ç¨‹ç¹æ‚ï¼Œæ‰€ä»¥éœ€è¦å¯¹æ¨¡å‹é¢„è®¾å‡ ç§è¶…å‚æ•°ç»„åˆã€‚**æ¯ç»„è¶…å‚æ•°éƒ½é‡‡ç”¨äº¤å‰éªŒè¯æ¥è¿›è¡Œè¯„ä¼°**ã€‚æœ€åé€‰å‡ºæœ€ä¼˜å‚æ•°ç»„åˆå»ºç«‹æ¨¡å‹ã€‚æœ€å¸¸ç”¨çš„äº¤å‰éªŒè¯ä¸º10æŠ˜äº¤å‰éªŒè¯.
![image](./images/6.png)

api: `sklearn.model_selection.GridSearchCV`

`sklearn.model_selection.GridSearchCV(estimator,Â param_grid=None,cv=None)`å¯¹ä¼°è®¡å™¨çš„æŒ‡å®šå‚æ•°å€¼è¿›è¡Œè¯¦å°½æœç´¢

- estimatorï¼šä¼°è®¡å™¨å¯¹è±¡
- param_gridï¼šä¼°è®¡å™¨å‚æ•°(dict){â€œn_neighborsâ€:[1,3,5]}
- cvï¼šæŒ‡å®šå‡ æŠ˜äº¤å‰éªŒè¯
- fitï¼šè¾“å…¥è®­ç»ƒæ•°æ®
- scoreï¼šå‡†ç¡®ç‡

ç»“æœåˆ†æï¼š

- best_score_:åœ¨äº¤å‰éªŒè¯ä¸­æµ‹è¯•çš„æœ€å¥½ç»“æœ
- best_estimator_ï¼šæœ€å¥½çš„å‚æ•°æ¨¡å‹
- cv_results_:æ¯æ¬¡äº¤å‰éªŒè¯åçš„æµ‹è¯•é›†å‡†ç¡®ç‡ç»“æœå’Œè®­ç»ƒé›†å‡†ç¡®ç‡ç»“æœ

```python
# ä½¿ç”¨ç½‘æ ¼æœç´¢, éœ€è¦æ³¨æ„çš„æ˜¯ä¸éœ€è¦ç»™å‚æ•°å¦åˆ™å‚æ•°ä¼šå›ºå®š
knn = KNeighborsClassifier()

# æ„é€ å‚æ•°çš„å€¼è¿›è¡Œæœç´¢
param = {"n_neighbors": [3, 5, 10]}

# è¿›è¡Œç½‘æ ¼æœç´¢
gc = GridSearchCV(knn, param_grid=param, cv=2)
gc.fit(x_train, y_train)

# é¢„æµ‹å‡†ç¡®ç‡
print("åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡:", gc.score(x_test, y_test))
print("åœ¨äº¤å‰éªŒè¯å½“ä¸­æœ€å¥½çš„ç»“æœ:", gc.best_score_)
print("æœ€å¥½çš„æ¨¡å‹(å‚æ•°):", gc.best_estimator_)
print("æ¯ä¸ªè¶…å‚æ•°æ¯æ¬¡äº¤å‰éªŒè¯çš„ç»“æœ", gc.cv_results_)
# åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡: 0.4739952718676123
# åœ¨äº¤å‰éªŒè¯å½“ä¸­æœ€å¥½çš„ç»“æœ: 0.44774590163934425
# æœ€å¥½çš„æ¨¡å‹(å‚æ•°): KNeighborsClassifier(n_neighbors=10)
# æ¯ä¸ªè¶…å‚æ•°æ¯æ¬¡äº¤å‰éªŒè¯çš„ç»“æœ ...
```

### åˆ†ç±»

åˆ†ç±»æ˜¯ä¸€ç§åŸºäºä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡ç¡®å®šå› å˜é‡æ‰€å±ç±»åˆ«çš„æŠ€æœ¯ã€‚

#### kè¿‘é‚»ç®—æ³•

å®šä¹‰ï¼šå¦‚æœä¸€ä¸ªæ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„kä¸ªæœ€ç›¸ä¼¼(å³ç‰¹å¾ç©ºé—´ä¸­æœ€é‚»è¿‘)çš„æ ·æœ¬ä¸­çš„å¤§å¤šæ•°å±äºæŸä¸€ä¸ªç±»åˆ«ï¼Œåˆ™è¯¥æ ·æœ¬ä¹Ÿå±äºè¿™ä¸ªç±»åˆ«ã€‚

> *æ¥æºï¼šKNNç®—æ³•æœ€æ—©æ˜¯ç”±Coverå’ŒHartæå‡ºçš„ä¸€ç§åˆ†ç±»ç®—æ³•*

ä¸¤ä¸ªæ ·æœ¬çš„è·ç¦»å¯ä»¥é€šè¿‡å¦‚ä¸‹å…¬å¼è®¡ç®—ï¼Œåˆå«æ¬§å¼è·ç¦».ç›¸ä¼¼çš„æ ·æœ¬ç‰¹å¾ä¹‹é—´çš„è·ç¦»ä¼šå¾ˆè¿‘.ä¸å…¶è·ç¦»æœ€è¿‘çš„ç‚¹æœ€ç›¸ä¼¼. æ¯”å¦‚è¯´ï¼Œa(a1,a2,a3),b(b1,b2,b3), é‚£ä¹ˆè·ç¦»å°±æ˜¯:

$$\sqrt{((ğ‘1âˆ’ğ‘1)^2+(ğ‘2âˆ’ğ‘2)^2+(ğ‘3âˆ’ğ‘3)^2)}$$

å› æ­¤, **kè¿‘é‚»ç®—æ³•ä¼šéœ€è¦åšæ ‡å‡†åŒ–å¤„ç†**

Api: `sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')`

- n_neighborsï¼šint,å¯é€‰ï¼ˆé»˜è®¤= 5ï¼‰ï¼Œk_neighborsæŸ¥è¯¢é»˜è®¤ä½¿ç”¨çš„é‚»å±…æ•°

- algorithmï¼š{â€˜autoâ€™ï¼Œâ€˜ball_treeâ€™ï¼Œâ€˜kd_treeâ€™ï¼Œâ€˜bruteâ€™}ï¼Œå¯é€‰ç”¨äºè®¡ç®—æœ€è¿‘é‚»å±…çš„ç®—æ³•ï¼šâ€˜ball_treeâ€™å°†ä¼šä½¿ç”¨ BallTreeï¼Œâ€˜kd_treeâ€™å°†ä½¿ç”¨ KDTreeã€‚â€˜autoâ€™å°†å°è¯•æ ¹æ®ä¼ é€’ç»™fitæ–¹æ³•çš„å€¼æ¥å†³å®šæœ€åˆé€‚çš„ç®—æ³•ã€‚ (ä¸åŒå®ç°æ–¹å¼å½±å“æ•ˆç‡)

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler


def knn_alg():
    """
    kè¿‘é‚»ç®—æ³•
    k-nearest neighbors algorithm
    """
    # è¯»å–æ•°æ®
    data = pd.read_csv("E:\\Workspace\\ml\\machine-learning-python\\data\\FBlocation\\train.csv")

    # ç¼©å°æ•°æ®, é€šè¿‡queryæŸ¥è¯¢æ•°æ®
    data = data.query("x > 1.0 & x < 1.25 & y > 2.5 & y < 2.75")

    # å¯¹æ—¶é—´è¿›è¡Œå¤„ç†
    time_value = pd.to_datetime(data['time'], unit='s')

    # æŠŠæ—¥æœŸæ ¼å¼è½¬æ¢æˆå­—å…¸æ ¼å¼
    time_value = pd.DatetimeIndex(time_value)
    print(time_value)
    # DatetimeIndex(['1970-01-01 18:09:40', '1970-01-10 02:11:10',
    #                '1970-01-05 15:08:02', '1970-01-06 23:03:03',
    #                '1970-01-09 11:26:50', '1970-01-02 16:25:07',
    #                ...

    # æ·»åŠ feature
    data['day'] = time_value.day
    data['hour'] = time_value.hour
    data['weekday'] = time_value.weekday
    # data.loc[:, 'day'] = time_value.day
    # data.loc[:, 'hour'] = time_value.hour
    # data.loc[:, 'weekday'] = time_value.weekday
    print(data)
    #             row_id       x       y  accuracy    time    place_id  day  hour  weekday
    # 600            600  1.2214  2.7023        17   65380  6683426742    1    18        3
    # 957            957  1.1832  2.6891        58  785470  6683426742   10     2        5

    # ä»dataä¸­åˆ é™¤æ—¶é—´ç‰¹å¾, 1è¡¨ç¤ºåˆ—, 0è¡¨ç¤ºè¡Œ
    data = data.drop(['time'], axis=1)
    print(data)
    #             row_id       x       y  accuracy    place_id  day  hour  weekday
    # 600            600  1.2214  2.7023        17  6683426742    1    18        3
    # 957            957  1.1832  2.6891        58  6683426742   10     2        5

    # æŠŠç­¾åˆ°æ•°é‡å°‘äºnä¸ªçš„ä½ç½®åˆ é™¤
    place_count = data.groupby('place_id').count()

    # place_count.row_idå°±æˆäº†countçš„è¿”å›å€¼äº†, ç„¶åæŠŠå¤§äº3çš„indexä¿ç•™ä½, ä¹Ÿå°±æ˜¯è¿‡æ»¤æ‰äº†å°äº3çš„id, ä¹Ÿå°±æ˜¯count
    # ç„¶åreset_index()å°±æ˜¯æŠŠindexå˜ä¸ºä¸€ä¸ªåˆ—,æ­¤å¤„å°±æ˜¯place_id,ä¹Ÿå°±æ˜¯åˆšåˆšçš„groupbyçš„åç§°è®¾ç½®ä¸ºä¸€ä¸ªåˆ—
    tf = place_count[place_count.row_id > 3].reset_index()

    # dataä¸­çš„place_idæ˜¯å¦åœ¨tf.place_idä¸­ä¹Ÿå°±æ˜¯åœ¨dataä¸­åˆ é™¤å°äº3çš„ç‰¹å¾å€¼
    data = data[data['place_id'].isin(tf.place_id)]

    # å–å‡ºæ•°æ®å½“ä¸­çš„ç›®æ ‡å€¼å’Œç‰¹å¾å€¼
    y = data['place_id']
    x = data.drop(['place_id'], axis=1)
    x = x.drop(['row_id'], axis=1)

    print(x)

    # åˆ†å‰²æ•°æ®
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

    # ç‰¹å¾å·¥ç¨‹(standarlize)
    std = StandardScaler()
    # å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç‰¹å¾å€¼è¿›è¡Œæ ‡å‡†åŒ–
    x_train = std.fit_transform(x_train)
    x_test = std.fit_transform(x_test)

    # ç®—æ³•, è®¡ç®—æœ€è¿‘çš„5ä¸ªç‚¹
    knn = KNeighborsClassifier(n_neighbors=5)

    knn.fit(x_train, y_train)

    # å¾—å‡ºé¢„æµ‹ç»“æœ
    y_predict = knn.predict(x_test)

    print("é¢„æµ‹çš„ç›®æ ‡ç­¾åˆ°ä½ç½®: ", y_predict)

    # å‡†ç¡®ç‡
    print("é¢„æµ‹çš„å‡†ç¡®ç‡", knn.score(x_test, y_test))

    return None


if __name__ == '__main__':
    knn_alg()

```

ä¼˜ç¼ºç‚¹:

- ç®€å•ï¼Œæ˜“äºç†è§£ï¼Œæ˜“äºå®ç°ï¼Œæ— éœ€ä¼°è®¡å‚æ•°ï¼Œæ— éœ€è®­ç»ƒ

- kå€¼å–å¾ˆå°ï¼šå®¹æ˜“å—å¼‚å¸¸ç‚¹å½±å“
- kå€¼å–å¾ˆå¤§ï¼šå®¹æ˜“å—æœ€è¿‘æ•°æ®å¤ªå¤šå¯¼è‡´æ¯”ä¾‹å˜åŒ–
- æ—¶é—´å¤æ‚åº¦å¾ˆé«˜, æ€§èƒ½å¾ˆå·®, æ‡’æƒ°ç®—æ³•ï¼Œå¯¹æµ‹è¯•æ ·æœ¬åˆ†ç±»æ—¶çš„è®¡ç®—é‡å¤§ï¼Œå†…å­˜å¼€é”€å¤§
- å¿…é¡»æŒ‡å®šKå€¼ï¼ŒKå€¼é€‰æ‹©ä¸å½“åˆ™åˆ†ç±»ç²¾åº¦ä¸èƒ½ä¿è¯

#### æœ´ç´ è´å¶æ–¯ç®—æ³•

æ¦‚ç‡å®šä¹‰ä¸ºä¸€ä»¶äº‹æƒ…å‘ç”Ÿçš„å¯èƒ½æ€§.

ç‰¹ç‚¹: æ²¡æœ‰å‚æ•°, ä¸éœ€è¦è°ƒæ•´å‚æ•°

ä¼˜ç‚¹ï¼š

- æœ´ç´ è´å¶æ–¯æ¨¡å‹å‘æºäºå¤å…¸æ•°å­¦ç†è®ºï¼Œæœ‰ç¨³å®šçš„åˆ†ç±»æ•ˆç‡ã€‚
- å¯¹ç¼ºå¤±æ•°æ®ä¸å¤ªæ•æ„Ÿï¼Œç®—æ³•ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œå¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚
- åˆ†ç±»å‡†ç¡®åº¦é«˜ï¼Œé€Ÿåº¦å¿«

ç¼ºç‚¹ï¼šéœ€è¦çŸ¥é“å…ˆéªŒæ¦‚ç‡P(F1,F2,â€¦|C)ï¼Œå› æ­¤åœ¨æŸäº›æ—¶å€™ä¼šç”±äºå‡è®¾çš„å…ˆéªŒæ¨¡å‹çš„åŸå› å¯¼è‡´é¢„æµ‹æ•ˆæœä¸ä½³ã€‚

##### æ¦‚ç‡ç†è®º

è”åˆæ¦‚ç‡ï¼šåŒ…å«å¤šä¸ªæ¡ä»¶ï¼Œä¸”æ‰€æœ‰æ¡ä»¶åŒæ—¶æˆç«‹çš„æ¦‚ç‡, è®°ä½œï¼š$ğ‘ƒ(ğ´,ğµ)$, ä¹Ÿå°±æ˜¯ $P(A,B) = P(A) * P(B)$

æ¡ä»¶æ¦‚ç‡ï¼šå°±æ˜¯äº‹ä»¶Aåœ¨å¦å¤–ä¸€ä¸ªäº‹ä»¶Bå·²ç»å‘ç”Ÿæ¡ä»¶ä¸‹çš„å‘ç”Ÿæ¦‚ç‡, è®°ä½œï¼š$ğ‘ƒ(ğ´|ğµ)$ ä¹Ÿå°±æ˜¯Bçš„æ¡ä»¶ä¸‹Açš„æ¦‚ç‡

Bå­˜åœ¨çš„æƒ…å†µä¸‹å‘ç”Ÿ$A_1$å’Œ$A_2$çš„æ¦‚ç‡ï¼š$P(A_1,A_2|B) = P(A_1|B)P(A_2|B)$
æ³¨æ„ï¼šæ­¤æ¡ä»¶æ¦‚ç‡çš„æˆç«‹çš„å‰ææ˜¯ç”±äºA1,A2ç›¸äº’ç‹¬ç«‹çš„ç»“æœ, ä¸å­˜åœ¨ç›¸äº’å½±å“

è´å¶æ–¯å…¬å¼
$${\displaystyle P(C\mid W)={\frac {P(C)P(W\mid C)}{P(W)}}}$$

> Wä¸ºç»™å®šæ–‡æ¡£çš„ç‰¹å¾å€¼(é¢‘æ•°ç»Ÿè®¡, é¢„æµ‹çš„æ–‡æ¡£), Cä¸ºæ–‡æ¡£ç±»åˆ«
> æ‰€ä»¥å…¬å¼å¯ä»¥ç†è§£ä¸º: è¯¥ç±»æ–‡ç« æ€»ä½“å‡ºç°çš„æ¦‚ç‡ * æ¯ä¸€ä¸ªè¯åœ¨è¯¥ç±»æ–‡ç« ä¸­å‡ºç°çš„æ¦‚ç‡ / æ¯ä¸€ä¸ªè¯åœ¨æ‰€æœ‰æ–‡ç« ä¸­å‡ºç°çš„æ¦‚ç‡

å…¬å¼å¯ä»¥å†™ä½œ

$$ P(C \mid F_1,F_2, ...)={\frac {P(F_1,F_2,... \mid C)P(C)}{P(F_1,F_2,...)}}$$

> **æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘**
> ç”±äºç»™å®šçš„æ•°å€¼å¯èƒ½ä¸º0, ä¸€æ—¦å‡ºç°, æ‰€æœ‰çš„æ¦‚ç‡ç»“æœè®¡ç®—éƒ½ä¼šä¸º0. æ‰€ä»¥å¯ä»¥ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°æ”¾å…¥è®¡ç®—ä¸­
> $${\hat {\theta }}_{i}ä¹Ÿå°±æ˜¯P(F_1 | C)={\frac {x_{i}+\alpha }{N+\alpha m}}\qquad (i=1,\ldots ,m),$$
> æ­¤å¤„çš„$\alpha$ä¸ºæŒ‡å®šçš„ç³»æ•°, ä¸€èˆ¬ä¸º1, $m$ä¸ºè®­ç»ƒæ–‡æ¡£ä¸­ç»Ÿè®¡å‡ºçš„ç‰¹å¾è¯ä¸ªæ•°
> æ­¤æ—¶çš„å‡½æ•°å°±å˜ä¸ºäº† æ–‡ç« ç‰¹å¾è¯æ•° + $\alpha$/æ–‡ç« è¯æ•° + $(\alpha * ç‰¹å¾è¯ç§ç±»)$ X (å…¶ä»–çš„ç‰¹å¾è®¡ç®—)...

##### ä»£ç å®ç°

æœ´ç´ è´å¶æ–¯api: `sklearn.naive_bayes.MultinomialNB(alpha = 1.0)` æ­¤å¤„çš„1.0å°±æ˜¯æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°. é»˜è®¤1.0

```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB


def bayes_algorithom():
    """
    æœ´ç´ è´å¶æ–¯åˆ†ç±»ç®—æ³•
    :return:
    """
    # ä¸‹è½½æ–°é—»æ•°æ®
    news = fetch_20newsgroups(subset='all')

    # è¿›è¡Œæ•°æ®åˆ†éš”, 25%çš„æµ‹è¯•æ•°æ®
    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25)

    # å¯¹æ•°æ®åŠè¿›è¡Œç‰¹å¾æŠ½å–
    tf = TfidfVectorizer()  # ä½¿ç”¨æ–‡æœ¬ç‰¹å¾æŠ½å–
    x_train = tf.fit_transform(x_train)  # é’ˆå¯¹æ¯ç¯‡æ–‡ç« çš„è¯è¿›è¡Œç»Ÿè®¡
    x_test = tf.transform(x_test)  # ä½¿ç”¨åŒæ ·çš„ç‰¹å¾æŠ½å–æµ‹è¯•é›†, å¹¶è¿›è¡Œç»Ÿè®¡, è¿™æ ·ç‰¹å¾æ•°é‡æ˜¯ç›¸åŒçš„

    print(tf.get_feature_names_out())

    # è¿›è¡Œæœ´ç´ è´å¶æ–¯ç®—æ³•è¿›è¡Œé¢„æµ‹
    mlt = MultinomialNB(alpha=1.0)
    print(x_train.toarray())
    mlt.fit(x_train, y_train)

    # å¾—å‡ºå‡†ç¡®ç‡
    y_predict = mlt.predict(x_test)
    print("é¢„æµ‹çš„æ–‡ç« ç±»åˆ«ä¸º: ", y_predict)
    print("å‡†ç¡®ç‡ä¸º: ", mlt.score(x_test, y_test))
    # é¢„æµ‹çš„æ–‡ç« ç±»åˆ«ä¸º:  [13 10  7 ... 15 15 10]
    # å‡†ç¡®ç‡ä¸º:  0.8552631578947368

    return None


if __name__ == '__main__':
    bayes_algorithom()
```

#### å†³ç­–æ ‘

å†³ç­–æ ‘æ€æƒ³çš„æ¥æºéå¸¸æœ´ç´ ï¼Œç¨‹åºè®¾è®¡ä¸­çš„æ¡ä»¶åˆ†æ”¯ç»“æ„å°±æ˜¯if-thenç»“æ„ï¼Œæœ€æ—©çš„å†³ç­–æ ‘å°±æ˜¯åˆ©ç”¨è¿™ç±»ç»“æ„åˆ†å‰²æ•°æ®çš„ä¸€ç§åˆ†ç±»å­¦ä¹ æ–¹æ³•

### å›å½’

### èšç±»
